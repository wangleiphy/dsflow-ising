\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{physics}
\usepackage{bm}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,decorations.pathmorphing}

\newcommand{\KL}{\mathrm{D}_{\mathrm{KL}}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\Ftrue}{F_{\mathrm{true}}}
\newcommand{\Fvar}{F_{\mathrm{var}}}
\newcommand{\sgn}{\operatorname{sign}}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\theoremstyle{remark}
\newtheorem{remark}{Remark}

\title{Discrete Normalizing Flows as Variational Ans\"atze\\for Classical Statistical Mechanics}
\author{Notes}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
We discuss the generalization of autoregressive variational ans\"atze for
classical statistical mechanics---originally formulated over raw spin
variables---to autoregressive models over \emph{transformed tokens}.  We
identify discrete normalizing flows as the natural framework that
preserves the key property of exact, tractable log-probabilities while
allowing learned, expressive transformations of the configuration space.
We describe the architecture, the joint training procedure, and discuss
the physical content that the learned flow may reveal, including
connections to the renormalization group, duality transformations,
topological defect encoding, and computational complexity of phases.
\end{abstract}

\tableofcontents

%======================================================================
\section{Background: autoregressive variational free energy}
\label{sec:background}
%======================================================================

Consider a classical spin system on a lattice of $N$ sites with
configuration $\bm{\sigma} = (\sigma_1, \ldots, \sigma_N)$, where each
$\sigma_i \in \{-1, +1\}$ (Ising case), and energy function
$E(\bm{\sigma})$.  The Boltzmann distribution at inverse temperature
$\beta = 1/T$ is
\begin{equation}
  p(\bm{\sigma}) = \frac{1}{Z} e^{-\beta E(\bm{\sigma})},
  \qquad
  Z = \sum_{\bm{\sigma}} e^{-\beta E(\bm{\sigma})}.
\end{equation}

Wu et al.~\cite{wu2019} proposed using autoregressive neural networks as
variational ans\"atze.  The variational distribution factorizes as
\begin{equation}
  q_\theta(\bm{\sigma})
  = \prod_{i=1}^{N} q_\theta(\sigma_i \mid \sigma_{<i}),
  \label{eq:ar-factorization}
\end{equation}
where each conditional is parameterized by a neural network (MADE,
PixelCNN, or RNN).  The variational free energy
\begin{equation}
  \Fvar[q]
  = \langle E(\bm{\sigma}) \rangle_q + T \langle \ln q(\bm{\sigma}) \rangle_q
  \geq \Ftrue
  \label{eq:var-free-energy}
\end{equation}
provides a rigorous upper bound on the true free energy $\Ftrue = -T \ln Z$.

The crucial property of \eqref{eq:ar-factorization} is that
$\ln q_\theta(\bm{\sigma}) = \sum_i \ln q_\theta(\sigma_i \mid
\sigma_{<i})$ is \emph{exact and tractable}, so both terms in
\eqref{eq:var-free-energy} can be estimated unbiasedly by sampling from
$q_\theta$.  Gradients are computed via the REINFORCE (policy gradient)
estimator:
\begin{equation}
  \nabla_\theta \Fvar
  = \E_{q_\theta}\!\Big[
    \big(E(\bm{\sigma}) + T \ln q_\theta(\bm{\sigma})\big)\,
    \nabla_\theta \ln q_\theta(\bm{\sigma})
  \Big].
  \label{eq:reinforce}
\end{equation}

%======================================================================
\section{Motivation: autoregression over transformed tokens}
\label{sec:motivation}
%======================================================================

The factorization~\eqref{eq:ar-factorization} is tied to a particular
ordering of the raw spin variables.  For systems with complex
correlation structures---especially near phase transitions, in frustrated
systems, or in the presence of topological defects---this raw-spin
factorization may be suboptimal.  One expects that a more natural set of
variables exists in which the autoregressive conditionals are simpler.

Several choices of transformed tokens are natural:
\begin{enumerate}[label=(\roman*)]
  \item \textbf{Block-spin tokens:} partition the lattice into blocks,
    label each block state as a single token.
  \item \textbf{Fourier-mode tokens:} factorize autoregressively in
    momentum space, generating modes from low-$k$ to high-$k$.
  \item \textbf{Wavelet tokens:} use a multiscale decomposition,
    generating coarse scales before fine scales.
  \item \textbf{Learned discrete tokens (VQ-VAE):} learn a discrete
    codebook via a vector-quantized autoencoder.
  \item \textbf{Domain-wall / defect tokens:} re-express configurations
    in terms of topological defects.
\end{enumerate}

The central question is whether such generalizations can maintain the
\emph{rigorous variational bound} of~\eqref{eq:var-free-energy}.

%======================================================================
\section{The rigour problem with latent-variable models}
\label{sec:rigour}
%======================================================================

The VQ-VAE approach defines a generative model with latent tokens
$\bm{z} = (z_1, \ldots, z_K)$:
\begin{equation}
  q(\bm{\sigma})
  = \sum_{\bm{z}} p_\theta(\bm{z})\, p_\phi(\bm{\sigma} \mid \bm{z}),
  \label{eq:vqvae-marginal}
\end{equation}
where $p_\theta(\bm{z}) = \prod_k p_\theta(z_k \mid z_{<k})$ is an
autoregressive prior and $p_\phi(\bm{\sigma} \mid \bm{z})$ is a
decoder.  The marginal~\eqref{eq:vqvae-marginal} is \emph{intractable}:
evaluating $\ln q(\bm{\sigma})$ requires summing over all latent
sequences.  This breaks the rigorous bound.

\subsection{Attempted fix: joint variational bound}

Introducing a reference distribution $r(\bm{z})$ and using
$\KL(q(\bm{\sigma}, \bm{z}) \,\|\, p_{\mathrm{Boltz}}(\bm{\sigma})
\cdot r(\bm{z})) \geq 0$ with $r(\bm{z}) = p_\theta(\bm{z})$ yields:
\begin{equation}
  \boxed{%
    \Ftrue \leq
    \langle E(\bm{\sigma}) \rangle_q
    + T \langle \ln p_\phi(\bm{\sigma} \mid \bm{z}) \rangle_q
  }
  \label{eq:joint-bound}
\end{equation}
This is a rigorous, tractable upper bound.  However, the gap between
\eqref{eq:joint-bound} and the true variational free energy $\Fvar[q]$
is
\begin{equation}
  \Delta = T \cdot I_q(\bm{\sigma};\, \bm{z}),
\end{equation}
the mutual information between spins and tokens under the joint model.
For a good generative model this mutual information is large
(potentially extensive in $N$), rendering the bound impractically loose.

\subsection{Attempted fix: ELBO + importance sampling}

Using an encoder $q_\psi(\bm{z} \mid \bm{\sigma})$, the standard ELBO
gives a \emph{lower} bound on $\ln q(\bm{\sigma})$:
\begin{equation}
  \ln q(\bm{\sigma})
  \geq \E_{q_\psi(\bm{z}|\bm{\sigma})}\!\big[
    \ln p_\phi(\bm{\sigma}|\bm{z})
    + \ln p_\theta(\bm{z})
    - \ln q_\psi(\bm{z}|\bm{\sigma})
  \big].
\end{equation}
Substituting this into~\eqref{eq:var-free-energy} gives a quantity
$\tilde{F} \leq \Fvar[q]$, which is \emph{not} a valid upper bound on
$\Ftrue$.  One can evaluate $q(\bm{\sigma})$ via importance sampling
after training, yielding a consistent estimator of $\Fvar[q]$, but not a
strict bound at finite sample size.

%======================================================================
\section{Discrete normalizing flows: the rigorous solution}
\label{sec:discrete-flows}
%======================================================================

The key insight is to replace the lossy VQ-VAE with a \emph{bijective}
discrete transformation.  Since the map is a bijection on a finite set,
no marginalization is needed and $\ln q(\bm{\sigma})$ remains exact.

\subsection{Setup}

Let $\bm{z} \in \{\pm 1\}^N$ denote latent spins and $\bm{\sigma} \in
\{\pm 1\}^N$ denote physical spins.  Define:
\begin{itemize}
  \item A \textbf{base distribution} $p_{\theta}(\bm{z}) = \prod_k
    p_{\theta}(z_k \mid z_{<k})$, an autoregressive model with
    parameters~$\theta$.
  \item A \textbf{discrete flow} $f_\phi : \{\pm 1\}^N \to \{\pm
    1\}^N$, a learnable bijection parameterized by~$\phi$.
\end{itemize}
The variational distribution over physical spins is
\begin{equation}
  q(\bm{\sigma})
  = p_{\theta}\!\big(f_\phi^{-1}(\bm{\sigma})\big),
  \label{eq:flow-distribution}
\end{equation}
and
\begin{equation}
  \ln q(\bm{\sigma})
  = \ln p_{\theta}(\bm{z})
  = \sum_{k=1}^{N} \ln p_{\theta}(z_k \mid z_{<k}),
  \qquad \bm{z} = f_\phi^{-1}(\bm{\sigma}).
  \label{eq:exact-logprob}
\end{equation}
No Jacobian correction is needed: for a bijection on a finite set, the
``Jacobian'' is identically~1.

\begin{proposition}
The variational free energy
$\Fvar[q] = \langle E(\bm{\sigma}) \rangle_q + T \langle \ln
q(\bm{\sigma}) \rangle_q$ with $q$ defined
by~\eqref{eq:flow-distribution} provides a rigorous upper bound on
$\Ftrue$ and can be evaluated exactly (up to sampling noise) via
\begin{equation}
  \Fvar = \E_{\bm{z} \sim p_{\theta}}\!\Big[
    E\!\big(f_\phi(\bm{z})\big)
    + T \ln p_{\theta}(\bm{z})
  \Big].
  \label{eq:Fvar-pullback}
\end{equation}
\end{proposition}

\subsection{Coupling layers for binary spins}
\label{sec:coupling}

We construct $f_\phi$ as a composition of $L$ \emph{discrete coupling
layers}, each of which is bijective by construction.

\paragraph{Single layer.}
Partition the $N$ sites into two groups $(A, B)$ (e.g., a checkerboard
partition on a square lattice).  A coupling layer acts as:
\begin{equation}
  \sigma_A = z_A, \qquad
  \sigma_B = z_B \odot m_\phi(z_A),
  \label{eq:coupling-layer}
\end{equation}
where $m_\phi : \{\pm 1\}^{|A|} \to \{\pm 1\}^{|B|}$ is a neural
network that outputs a conditional \emph{flip mask}, and $\odot$ denotes
elementwise multiplication.  Since $(\pm 1)^2 = +1$, the inverse is
identical:
\begin{equation}
  z_B = \sigma_B \odot m_\phi(\sigma_A), \qquad z_A = \sigma_A.
\end{equation}

\paragraph{Composition.}
The full flow is
\begin{equation}
  f_\phi = f^{(L)} \circ f^{(L-1)} \circ \cdots \circ f^{(1)},
\end{equation}
alternating which sites belong to group~$A$ and group~$B$ at each layer.
Each $f^{(l)}$ has its own mask network with parameters $\phi^{(l)}
\subset \phi$.

\paragraph{Algebraic interpretation.}
In the $\F_2$ representation ($\{0,1\}$ with XOR), each coupling layer
implements a conditional affine transformation: $\sigma_B = z_B \oplus
m_\phi(z_A)$.  The space of all such compositions forms a subgroup of
$\mathrm{Aut}(\F_2^N)$.  The flow searches for an element of this group
such that the Boltzmann distribution, expressed in the coordinates
$\bm{z} = f_\phi^{-1}(\bm{\sigma})$, admits the simplest
autoregressive factorization.


\subsection{Enforcing $\mathbb{Z}_2$ spin-flip symmetry}
\label{sec:z2-symmetry}

The Ising Hamiltonian is invariant under the global spin flip
$\bm{\sigma} \to -\bm{\sigma}$, so the Boltzmann distribution satisfies
$p(\bm{\sigma}) = p(-\bm{\sigma})$.  A variational ansatz that respects
this $\mathbb{Z}_2$ symmetry exactly has two advantages: (i) it halves
the effective configuration space the model must learn, and (ii) it
prevents mode collapse to a single magnetisation sector.

\paragraph{$\mathbb{Z}_2$-symmetric base distribution.}
Given a bare autoregressive model $p_\theta^{\mathrm{AR}}(\bm{z})$, we
define the symmetrised base distribution
\begin{equation}
  p_\theta(\bm{z})
  = \tfrac{1}{2}\, p_\theta^{\mathrm{AR}}(\bm{z})
  + \tfrac{1}{2}\, p_\theta^{\mathrm{AR}}(-\bm{z}),
  \label{eq:z2-base}
\end{equation}
which satisfies $p_\theta(\bm{z}) = p_\theta(-\bm{z})$ by construction.
The log-probability is evaluated via the log-sum-exp identity:
\begin{equation}
  \ln p_\theta(\bm{z})
  = \mathrm{logsumexp}\!\big(
    \ln p_\theta^{\mathrm{AR}}(\bm{z}),\;
    \ln p_\theta^{\mathrm{AR}}(-\bm{z})
  \big) - \ln 2.
  \label{eq:z2-logprob}
\end{equation}
Sampling proceeds by drawing $\bm{z} \sim p_\theta^{\mathrm{AR}}$ and
then flipping all spins with probability~$\tfrac{1}{2}$.

\paragraph{$\mathbb{Z}_2$-equivariant flow.}
For the full variational distribution $q(\bm{\sigma}) =
p_\theta(f_\phi^{-1}(\bm{\sigma}))$ to inherit the symmetry, the flow
must be \emph{equivariant}: $f_\phi(-\bm{z}) = -f_\phi(\bm{z})$.
Combined with~\eqref{eq:z2-base}, this gives
\begin{equation}
  q(-\bm{\sigma})
  = p_\theta\!\big(f_\phi^{-1}(-\bm{\sigma})\big)
  = p_\theta\!\big(-f_\phi^{-1}(\bm{\sigma})\big)
  = p_\theta\!\big(f_\phi^{-1}(\bm{\sigma})\big)
  = q(\bm{\sigma}).
\end{equation}

\paragraph{Equivariant coupling layers.}
Recall that a coupling layer acts as $\sigma_A = z_A$ and $\sigma_B =
z_B \odot m_\phi(z_A)$.  Under $\bm{z} \to -\bm{z}$:
\begin{itemize}
  \item The $A$-sublattice maps to $-z_A$ (passthrough), giving
    $\sigma_A = -z_A$.
  \item The $B$-sublattice gives $\sigma_B = (-z_B) \odot m_\phi(-z_A)$.
\end{itemize}
For equivariance we need $\sigma_B = -z_B \odot m_\phi(z_A)$, which
requires
\begin{equation}
  m_\phi(-z_A) = m_\phi(z_A),
  \label{eq:even-mask}
\end{equation}
i.e., the mask network must be an \emph{even function} of its input.

\paragraph{Symmetrised mask network.}
For any neural network $h_\phi$, define
\begin{equation}
  g_\phi(x) = h_\phi(x) + h_\phi(-x).
  \label{eq:symmetrised-mask}
\end{equation}
Then $g_\phi(-x) = h_\phi(-x) + h_\phi(x) = g_\phi(x)$, so
condition~\eqref{eq:even-mask} is satisfied exactly.  The mask is then
$m_\phi(z_A) = \sgn(g_\phi(z_A))$.  This construction doubles the
computational cost of the mask network (two forward passes per layer) but
imposes no restriction on the network architecture~$h_\phi$.

\begin{remark}
For $\pm 1$ spins, one might consider using $|z_A|$ or $z_A^2$ as
input to enforce evenness.  However, since $|z_i| = z_i^2 = 1$ for all
$z_i \in \{\pm 1\}$, these are constant and carry no information.  The
symmetrisation~\eqref{eq:symmetrised-mask} is the simplest approach that
preserves the full expressiveness of the mask network.
\end{remark}

\subsection{Physical interpretation of a coupling layer}
\label{sec:physical-interp}

The algebraic definition~\eqref{eq:coupling-layer} has a transparent
physical meaning.  Consider a square lattice with the checkerboard
partition: group~$A$ is one sublattice (say the black squares) and
group~$B$ is the other (white squares).  The coupling layer examines
\emph{all} the black spins and, for each white spin independently,
decides whether to flip it.  The mask network $m_\phi(z_A)$ encodes
this decision rule.

\paragraph{As a deterministic sublattice update.}
The simplest useful mask aligns each white spin with its local field
from the black neighbours.  If site $i \in B$ has local field
$h_i = J \sum_{j \in A,\, j \sim i} z_j$, then
\begin{equation}
  m_i = \sgn(h_i)
  \label{eq:mean-field-mask}
\end{equation}
flips $z_{B,i}$ to point along the local field.  This is a
\emph{deterministic mean-field update on one sublattice}---one
half-sweep of iterative conditional modes.  The learned mask network can
go well beyond this: it sees the \emph{entire} $A$-sublattice
configuration and can make non-local, context-dependent flip decisions.

\paragraph{Alternating refinement.}
Stacking layers with alternating partitions yields the structure shown
in Table~\ref{tab:layers}.  Each layer refines one sublattice
conditioned on the other---\emph{deterministic alternating Gibbs
sampling} with learned update rules.

\begin{table}[h]
\centering
\caption{Alternating sublattice updates in a composition of coupling layers.}
\label{tab:layers}
\begin{tabular}{lccc}
\toprule
\textbf{Layer} & \textbf{Reads} & \textbf{Updates} & \textbf{Correlations resolved} \\
\midrule
1 & black sublattice & white sublattice & nearest-neighbour $A$--$B$ \\
2 & white (corrected) & black sublattice & next-nearest-neighbour $A$--$A$ via $B$ \\
3 & black (corrected) & white sublattice & $\sim 3$ lattice spacings \\
$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
$l$ & --- & --- & $\sim l$ lattice spacings \\
\bottomrule
\end{tabular}
\end{table}

The correlation range captured by the flow grows linearly with
depth:
\begin{equation}
  \xi_{\mathrm{eff}} \sim L_{\mathrm{flow}}.
  \label{eq:xi-vs-depth}
\end{equation}
This is the direct origin of the scaling $L^\ast \sim \xi \sim |T -
T_c|^{-\nu}$ discussed in Sec.~\ref{sec:physics}: the flow must be at
least as deep as the correlation length is long.

\paragraph{Action on defects.}
The inverse flow $f_\phi^{-1}$ maps physical configurations to the
latent space.  Its action on defects is instructive:
\begin{itemize}
  \item \textbf{Smooth domains} (all spins aligned) pass through nearly
    unchanged---all masks are close to $+1$.
  \item \textbf{Domain walls} are progressively straightened, shrunk,
    or moved to a canonical position.  Each layer peels away one
    ``layer of roughness'' from the wall.
  \item \textbf{Point defects} (a single flipped spin in a uniform
    background) are absorbed: the mask flips them back, mapping the
    configuration to the uniform state in $\bm{z}$-space.
\end{itemize}
The forward flow does the reverse: starting from a simple latent
configuration, it \emph{grows} defects layer by layer---first placing
them roughly, then refining their shape and position.

\paragraph{Frustration increases per-layer difficulty.}
For an unfrustrated bipartite lattice (e.g.\ the ferromagnetic square
Ising model), each $B$-spin wants to align with all its $A$-neighbours
and there is no conflict; the local-field mask~\eqref{eq:mean-field-mask}
already does a good job.  For a frustrated system (triangular
antiferromagnet, spin glass), each $B$-spin receives contradictory
signals from its $A$-neighbours.  The mask network must learn a
complex, non-local compromise, requiring larger networks and deeper
flows---reflecting the intrinsic computational hardness of the
frustrated phase.

\paragraph{Summary.}
A discrete coupling layer is a \emph{learned, deterministic,
sublattice-conditional spin update}: the discrete analogue of one
half-sweep of Gibbs sampling, optimised end-to-end so that the full
composition maps a simple base distribution to the target Boltzmann
distribution.

%======================================================================
\section{Joint training procedure}
\label{sec:training}
%======================================================================

The trainable parameters are $\theta$ (base AR model) and $\phi$
(flow coupling networks).  The objective~\eqref{eq:Fvar-pullback} is
minimized by gradient descent.  The two parameter groups play very
different roles, and their gradients have different structures.

\subsection{Gradient with respect to $\theta$ (base AR model)}

Both the sampling distribution and the integrand depend on $\theta$.
Applying the REINFORCE identity:
\begin{equation}
  \nabla_{\theta} \Fvar
  = \E_{p_{\theta}}\!\bigg[
    \Big(E\!\big(f_\phi(\bm{z})\big)
    + T \ln p_{\theta}(\bm{z})\Big)\;
    \nabla_{\theta} \ln p_{\theta}(\bm{z})
  \bigg].
  \label{eq:grad-theta}
\end{equation}
This is the standard policy-gradient estimator from~\cite{wu2019}.
Variance reduction via a learned baseline $b(\bm{z}_{<k})$ is essential:
\begin{equation}
  \nabla_{\theta} \Fvar
  \approx \frac{1}{M} \sum_{m=1}^{M}
  \Big(R^{(m)} - b\Big)\,
  \nabla_{\theta} \ln p_{\theta}(\bm{z}^{(m)}),
  \qquad
  R^{(m)} = E(\bm{\sigma}^{(m)}) + T \ln p_{\theta}(\bm{z}^{(m)}).
\end{equation}

\subsection{Gradient with respect to $\phi$ (flow parameters)}

This gradient has a qualitatively different structure.  Recall the
objective pulled back to latent space:
\begin{equation}
  \Fvar
  = \E_{\bm{z} \sim p_\theta}\!\Big[
    \underbrace{E\!\big(f_\phi(\bm{z})\big)}_{\text{depends on } \phi}
    + \underbrace{T \ln p_\theta(\bm{z})}_{\text{independent of } \phi}
  \Big].
  \label{eq:Fvar-phi-view}
\end{equation}
Two key observations:
\begin{enumerate}
  \item The sampling distribution $p_\theta(\bm{z})$ does not depend on
    $\phi$, so we can move $\nabla_\phi$ inside the expectation without
    a REINFORCE correction.
  \item The entropy term $T \ln p_\theta(\bm{z})$ does not depend on
    $\phi$ at all---the flow does not change which latent configuration
    was drawn, only where it lands in physical space.
\end{enumerate}
Therefore:
\begin{equation}
  \nabla_\phi \Fvar
  = \E_{p_{\theta}}\!\Big[
    \nabla_\phi E\!\big(f_\phi(\bm{z})\big)
  \Big].
  \label{eq:grad-phi}
\end{equation}
The entropy drops out entirely: \emph{the flow is optimised purely by
rearranging which latent configuration maps to which physical
configuration, so as to lower the expected energy.}

\paragraph{The discrete barrier.}
Equation~\eqref{eq:grad-phi} looks like a standard backpropagation
problem, but there is a fundamental obstacle.  Inside each coupling
layer, the mask is computed as
\begin{equation}
  m_i = \sgn\!\big(g_\phi(z_A)_i\big) \in \{\pm 1\},
\end{equation}
where $g_\phi$ is a neural network with continuous outputs.  As $\phi$
varies continuously, $g_\phi$ changes smoothly, but $\sgn(\cdot)$ snaps
to $\pm 1$.  The composed map $f_\phi(\bm{z})$ is therefore a
\emph{piecewise-constant} function of $\phi$: it takes discrete values
and is flat almost everywhere, with discontinuous jumps at the boundaries
where some $g_\phi(z_A)_i$ crosses zero.  Consequently,
$\nabla_\phi E(f_\phi(\bm{z})) = 0$ almost everywhere in the
conventional sense---standard backpropagation yields no gradient signal.

We employ one of the following relaxations to obtain a useful training
signal:

\paragraph{Straight-through estimator (STE).}
In the forward pass, compute hard masks $m_i = \sgn(g_\phi(z_A)_i)$.
In the backward pass, replace $\sgn'$ by the identity, so that gradients
flow through the mask network as if $m_i \approx g_\phi(z_A)_i$:
\begin{equation}
  \frac{\partial m_i}{\partial g_i}\bigg|_{\mathrm{STE}} = 1.
\end{equation}
This is biased but empirically effective.  Gradients backpropagate
through the full composition $f^{(L)} \circ \cdots \circ f^{(1)}$.

\paragraph{Gumbel--softmax relaxation.}
During training, replace the hard mask with a continuous surrogate:
\begin{equation}
  \tilde{m}_i
  = \tanh\!\bigg(
    \frac{g_\phi(z_A)_i + (\xi_1 - \xi_2)}{\tau}
  \bigg),
  \qquad \xi_{1,2} \sim \mathrm{Gumbel}(0,1),
\end{equation}
and anneal $\tau \to 0$ over training.  At $\tau = 0$, the exact
discrete flow is recovered.

\paragraph{Decoupling property.}
A key simplification: the gradient for $\theta$ does not require
differentiating through the flow, and the gradient for $\phi$ does not
require REINFORCE.  The two parameter groups can be updated with
different optimizers and learning rates.

\subsection{Training algorithm}

\begin{algorithm}[H]
\caption{Joint training of autoregressive base + discrete flow}
\label{alg:training}
\begin{algorithmic}[1]
\Require Base AR network $p_{\theta}$, flow layers $\{f^{(l)}_\phi\}_{l=1}^L$, temperature $T$, batch size $M$
\For{each training step}
  \State Sample $\bm{z}^{(m)} \sim p_{\theta}(\bm{z})$ for $m = 1, \ldots, M$
    \hfill\Comment{Autoregressive sampling}
  \State Compute $\bm{\sigma}^{(m)} = f_\phi(\bm{z}^{(m)})$
    \hfill\Comment{Forward through flow (hard masks)}
  \State Compute $\ln q^{(m)} = \sum_k \ln p_{\theta}(z^{(m)}_k \mid z^{(m)}_{<k})$
    \hfill\Comment{Exact log-probability}
  \State Compute local free energy $R^{(m)} = E(\bm{\sigma}^{(m)}) + T \ln q^{(m)}$
  \State \textbf{Update $\theta$:} $\theta \leftarrow \theta - \alpha_\theta \cdot \frac{1}{M}\sum_m (R^{(m)} - b)\, \nabla_{\theta}\ln p_{\theta}(\bm{z}^{(m)})$
    \hfill\Comment{REINFORCE}
  \State \textbf{Update $\phi$:} $\phi \leftarrow \phi - \alpha_\phi \cdot \frac{1}{M}\sum_m \nabla_\phi E(\tilde{f}_\phi(\bm{z}^{(m)}))$
    \hfill\Comment{STE backward pass}
\EndFor
\end{algorithmic}
\end{algorithm}

%======================================================================
\section{Physical content of the learned flow}
\label{sec:physics}
%======================================================================

The learned bijection $f_\phi$ is an interpretable object: it defines a
\emph{change of variables} in configuration space that the model finds
optimal for representing the Boltzmann distribution.  We discuss several
ways in which this transformation encodes non-trivial physics.

\subsection{Emergent renormalization group}

If the flow is structured hierarchically---e.g., layer~1 acts on
nearest-neighbor pairs, layer~2 on $2\times 2$ blocks, layer~3 on
$4\times 4$ blocks---then the composition implements a multiscale
transformation.  The base distribution captures an effective theory at
the coarsest scale.

\paragraph{Diagnostic 1: self-similarity at criticality.}
At the RG fixed point $T = T_c$, the flow parameters should become
approximately self-similar across scales: the mask networks at different
layers should implement statistically similar transformations.  Away from
$T_c$, deep layers contribute little because correlations are
short-range, and the flow effectively ``terminates early.''

\paragraph{Diagnostic 2: effective degrees of freedom.}
The base-distribution entropy $H[p_{\theta}]$ at each scale gives an
estimate of the effective number of degrees of freedom, analogous to the
$c$-function in the Zamolodchikov $c$-theorem.  One can track how
$H[p_{\theta}]$ varies with temperature and system size to extract
scaling exponents.

\subsection{Kramers--Wannier duality}

The 2D Ising model on a square lattice possesses a duality
transformation $\mathcal{D}$ that maps high-temperature configurations
(spins) to low-temperature configurations (domain walls), with the
self-dual point at $T_c$.

\begin{itemize}
  \item A flow trained at $T_c$ should approximate $\mathcal{D}$ (or its
    composition with a simple transformation), since the self-dual
    distribution has enhanced symmetry that the flow can exploit.
  \item One can test this by examining the flow's action on known
    configurations (all-up, checkerboard, single domain wall) and
    comparing with the analytical Kramers--Wannier map.
  \item More generally, for models with known dualities (Potts, gauge--Higgs),
    the learned flow may \emph{rediscover} the duality transformation,
    providing a data-driven route to non-obvious dualities.
\end{itemize}

\subsection{Topological defect encoding}

In ordered phases, the dominant excitations are topological defects
(domain walls in the Ising model, vortices in the XY model, monopoles in
gauge theories).  A well-trained flow should map configurations to a
latent space where:
\begin{enumerate}
  \item A small number of latent variables encode the \emph{number and
    topology} of defects (their winding numbers, connectivity, etc.).
  \item The remaining latent variables encode \emph{smooth deformations}
    of defect positions and shapes.
\end{enumerate}
This can be tested by feeding configurations with known defect content
through $f_\phi^{-1}$ and examining the latent representation.
Configurations with the same defect topology should map to nearby regions
in $\bm{z}$-space.

\subsection{Flow depth as a probe of computational complexity}

The minimum flow depth $L^\ast$ required to achieve a given free-energy
accuracy is a measure of the \emph{circuit complexity} of the Boltzmann
distribution.

\paragraph{Unfrustrated systems.}
For the ferromagnetic Ising model, we expect $L^\ast \sim \xi$, where
$\xi \sim |T - T_c|^{-\nu}$ is the correlation length.  At $T_c$,
$L^\ast$ diverges with system size as $L^\ast \sim N^{\nu/d}$.

\paragraph{Frustrated / glassy systems.}
For the Sherrington--Kirkpatrick model or other spin glasses, $L^\ast$
may grow much faster---potentially exponentially in $N$---reflecting the
NP-hardness of the ground-state problem.  Mapping out $L^\ast(T, N)$ as
a ``computational phase diagram'' could reveal transitions in
computational complexity that coincide with (or differ from)
thermodynamic phase boundaries.

\subsection{Disentanglement of order parameter and fluctuations}

Near a phase transition, the physically relevant decomposition is:
\begin{equation}
  \bm{\sigma}
  = \underbrace{\bar{\bm{\sigma}}(\text{order parameter})}_{
    \text{first few } z_k}
  + \underbrace{\delta\bm{\sigma}(\text{fluctuations})}_{
    \text{remaining } z_k}.
\end{equation}
If the base AR model generates the first few $z_k$ first, one can check:
\begin{itemize}
  \item Do the leading latent variables encode the magnetization?
  \item Is the conditional $p(z_{k+1}, \ldots \mid z_1, \ldots, z_k)$
    approximately Gaussian for intermediate~$k$ (Landau--Ginzburg regime)?
  \item Does the mutual-information bottleneck---identifying which latent
    variables carry the most information about the energy---shift at
    $T_c$?
\end{itemize}

\subsection{The flow as non-equilibrium dynamics}

The flow $f_\phi$ defines a deterministic map from a simple
(high-temperature-like) base distribution to the target Boltzmann
distribution.  Layer by layer, it ``cools'' the system.  The
intermediate distributions
\begin{equation}
  q_l(\bm{\sigma})
  = p_{\theta}\!\big((f^{(l)} \circ \cdots \circ f^{(1)})^{-1}(\bm{\sigma})\big),
  \qquad l = 1, \ldots, L,
\end{equation}
trace out a path in the space of probability distributions.  One can
measure the free-energy change at each layer,
\begin{equation}
  \Delta F_l = \Fvar[q_l] - \Fvar[q_{l-1}],
\end{equation}
and study whether the flow finds a quasi-static (reversible) path or a
non-equilibrium shortcut.  This has natural connections to Jarzynski's
equality and optimal transport.

%======================================================================
\section{Case study: the 2D Ising model at criticality}
\label{sec:critical-ising}
%======================================================================

We now specialise the general framework to the square-lattice Ising model
at $T_c = 2J / \ln(1 + \sqrt{2}) \approx 2.269\, J/k_B$ and develop
concrete expectations for what the flow and the latent variables should
learn.  This system is exactly solvable and described by the $c = 1/2$
minimal-model conformal field theory (CFT), so sharp predictions are
possible.

\subsection{The challenge: no characteristic scale}

Away from $T_c$, correlations decay as $\langle \sigma_i \sigma_j
\rangle \sim e^{-|i-j|/\xi}$ with finite $\xi$, and a flow of depth
$L \gtrsim \xi$ can capture all correlations.  At $T_c$, correlations
are power-law,
\begin{equation}
  \langle \sigma_i \sigma_j \rangle \sim |i - j|^{-\eta},
  \qquad \eta = \tfrac{1}{4},
\end{equation}
and $\xi \to \infty$.  No finite-depth flow suffices: every layer
matters, and the flow can never fully decorrelate the latent variables.

\subsection{What the flow does: progressive smoothing of fractal domain walls}

At $T_c$, domain walls are fractal curves described by
$\mathrm{SLE}_3$ (Schramm--Loewner evolution with $\kappa = 3$, fractal
dimension $d_f = 11/8$).  The inverse flow $f_\phi^{-1} : \bm{\sigma}
\to \bm{z}$ acts as a progressive \emph{simplifier}:
\begin{itemize}
  \item \textbf{Layer~1} (reads black sublattice, flips white): removes
    roughness at the 1-lattice-spacing scale.  Domain walls become
    slightly smoother.
  \item \textbf{Layer~2} (reads white, flips black): removes roughness
    at the 2-spacing scale.
  \item \textbf{Layer~$l$}: removes fluctuations at scale $\sim l$.
\end{itemize}
After $L$ layers, the latent configuration $\bm{z}$ looks like a
\emph{coarse-grained} version of $\bm{\sigma}$: the fractal domain
walls have been straightened up to scale~$L$, but their large-scale
topology is preserved.

The forward flow $f_\phi : \bm{z} \to \bm{\sigma}$ does the reverse:
starting from the smooth latent configuration, it progressively
\emph{roughens} the domain walls, adding fractal detail at finer and
finer scales---a constructive, layer-by-layer assembly of the critical
microstate.

\subsection{Self-similarity of the flow at $T_c$}

Because the critical Ising model is scale-invariant, the fluctuations at
scale~$l$ are statistically identical to those at scale~$l+1$ (up to
rescaling).  This implies:
\begin{enumerate}
  \item The mask networks at different layers should learn
    \textbf{statistically similar transformations}---each layer does the
    ``same job'' at a different scale.
  \item The free-energy reduction per layer,
    \begin{equation}
      \Delta F_l = \Fvar[q_l] - \Fvar[q_{l-1}],
    \end{equation}
    should be approximately \textbf{constant} across layers (or decay as
    a slow power law $\sim l^{-(2-\eta)}$), reflecting the equal
    importance of all scales.
  \item Away from $T_c$, $\Delta F_l$ drops sharply for $l > \xi$---deep
    layers become idle.  At $T_c$, \emph{no layer is idle}.
\end{enumerate}
The constant $\Delta F_l$ at criticality is the flow-level signature of
scale invariance.

\subsection{Division of labour between flow and base}

The flow and the base AR model split the representational work along a
\textbf{scale axis}:
\begin{equation}
  \underbrace{p_\theta(\bm{z})}_{\substack{\text{IR physics:}\\
    \text{global topology,}\\ \text{order parameter}}}
  \;\xrightarrow{\;f_\phi\;}
  \underbrace{q(\bm{\sigma})}_{\text{full critical distribution}}.
\end{equation}
\begin{itemize}
  \item The \textbf{flow} $f_\phi$ handles correlations at scales
    $\lesssim L_{\mathrm{flow}}$: local spin alignment, domain-wall
    geometry, short-range order.  This is the \emph{UV} (ultraviolet)
    physics.
  \item The \textbf{base} $p_\theta(\bm{z})$ handles correlations at
    scales $\gtrsim L_{\mathrm{flow}}$: the global magnetisation sector,
    large-scale domain topology, long-range order-parameter correlations.
    This is the \emph{IR} (infrared) physics.
\end{itemize}
The flow is a \textbf{constructive renormalisation group}: it runs the RG
\emph{backwards}, starting from the coarse (IR) description encoded in
$\bm{z}$ and progressively adding fine (UV) detail.

\subsection{Physical meaning of the latent variables}

The autoregressive base generates $\bm{z} = (z_1, z_2, \ldots, z_N)$
sequentially.  The first variables condition everything that follows, so
the model assigns them the most important, most informative features.

\paragraph{Early variables ($z_1, z_2, \ldots, z_{\mathrm{few}}$):
global structure.}
\begin{itemize}
  \item $z_1$ should encode the \textbf{global $\mathbb{Z}_2$ sector}---the
    overall sign of the magnetisation.  At $T_c$,
    $p(z_1 = +1) \approx p(z_1 = -1) \approx 1/2$ (maximal
    uncertainty).  This single bit is the most informative feature of a
    critical configuration.
  \item The next several $z_k$ encode the \textbf{large-scale domain
    topology}: how many major domains exist, their rough spatial
    arrangement, the coarse shape of the dominant domain wall.
\end{itemize}

\paragraph{Middle variables: progressive refinement.}
These encode the domain-wall geometry at progressively finer scales---the
``wavelet coefficients'' of the magnetisation field at intermediate
wavelengths.  Each variable adds detail at a specific scale, like
successive terms in a multiscale expansion.

\paragraph{Late variables ($z_{N-\mathrm{few}}, \ldots, z_N$): thermal
noise.}
These are nearly independent of all preceding variables:
$p(z_k \mid z_{<k}) \approx \mathrm{Bernoulli}(1/2)$.  They encode the
finest-scale fluctuations that the flow could not fully decorrelate---the
residual ``thermal noise.''

\subsection{The conditional entropy profile as a diagnostic}

The conditional entropy $H(z_k \mid z_{<k})$ measures how much new
information each latent variable adds.  Its profile is a sharp diagnostic
of the phase:

\paragraph{At $T_c$ (critical).}
$H(z_1) = \ln 2$ (the $\mathbb{Z}_2$ choice is maximally uncertain).
$H(z_k \mid z_{<k})$ then decreases \textbf{slowly}---as a power
law---because scale-invariant correlations mean there is always more
structure to specify at the next scale.  The information is spread across
all scales.

\paragraph{Below $T_c$ (ordered).}
$H(z_1) \ll \ln 2$ (the magnetisation is nearly determined).  The
remaining conditional entropies drop quickly to $\approx \ln 2$ as the
variables become trivial fluctuations around the ordered state.

\paragraph{Above $T_c$ (disordered).}
$H(z_k \mid z_{<k}) \approx \ln 2$ for all~$k$---the variables are
nearly independent.  The base is close to a product distribution, and the
flow does little.

The \textbf{slow power-law decay of $H(z_k \mid z_{<k})$ at $T_c$} is
the autoregressive fingerprint of criticality.

\subsection{Connection to the CFT operator content}

The critical 2D Ising model is described by the $c = 1/2$ minimal-model
CFT with primary operators:
\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Operator} & \textbf{Conformal dimension $h$}
  & \textbf{Physical meaning} \\
\midrule
$\mathbb{1}$ & 0 & Identity \\
$\sigma$ & $1/16$ & Spin (order parameter) \\
$\varepsilon$ & $1/2$ & Energy density \\
\bottomrule
\end{tabular}
\end{center}
The most relevant operator (lowest~$h$) is the spin field~$\sigma$.  The
latent variables should encode the amplitudes of these operators in a
hierarchical way:
\begin{itemize}
  \item \textbf{First latent variables}: amplitude of the spin
    field~$\sigma$ at the longest wavelength---this is the
    magnetisation.  The low conformal dimension $h = 1/16$ means it is
    the most slowly decaying, most important mode.
  \item \textbf{Next variables}: amplitude of the energy
    density~$\varepsilon$ at long wavelengths, and the spin field at
    shorter wavelengths.
  \item \textbf{Deeper variables}: descendant operators (spatial
    derivatives of primaries), encoding finer spatial structure.
\end{itemize}
The autoregressive ordering effectively implements a \textbf{spectral
decomposition} of the critical distribution, ordered by relevance
(conformal dimension).

\subsection{What the latent configuration looks like}

If one visualises $\bm{z}$ on the lattice:
\begin{itemize}
  \item \textbf{At $T_c$}: $\bm{z}$ should resemble a
    \textbf{smoothed, coarse-grained} version of $\bm{\sigma}$.  The
    large-scale domain structure is visible, but the fractal roughness of
    domain walls has been removed.  It looks like the output of a
    block-spin RG transformation applied $\sim L_{\mathrm{flow}}$ times.
  \item \textbf{Below $T_c$}: $\bm{z}$ is nearly uniform (all $+1$ or
    all $-1$), since the flow can build the few thermal excitations from
    a simple base.
  \item \textbf{Above $T_c$}: $\bm{z} \approx \bm{\sigma}$---the flow
    has little to do, and the latent and physical configurations are
    nearly identical.
\end{itemize}

\subsection{Summary: the flow as a constructive RG}

\begin{equation}
  \underbrace{z_1}_{\mathbb{Z}_2\text{ sector}}
  \;\to\;
  \underbrace{z_2, \ldots, z_k}_{\text{large-scale topology}}
  \;\to\;
  \underbrace{z_{k+1}, \ldots, z_K}_{\text{medium-scale detail}}
  \;\to\;
  \underbrace{z_{K+1}, \ldots, z_N}_{\text{thermal noise}}
  \;\xrightarrow{\;f_\phi\;}
  \;\bm{\sigma}
\end{equation}
The base AR model generates a coarse description of the critical
configuration, ordered from the most relevant (IR) to the least relevant
(UV) degrees of freedom.  The flow then dresses this coarse description
with the geometric detail at all scales up to the lattice spacing.  At
$T_c$, both components are maximally stressed: the base must capture
power-law correlations, and the flow must build self-similar fractal
structure at every scale.

%======================================================================
\section{Preliminary numerical results}
\label{sec:results}
%======================================================================

We report results from a JAX/Flax implementation of the discrete flow
framework applied to the 2D ferromagnetic Ising model on a $32 \times 32$
square lattice with periodic boundary conditions at $T_c \approx 2.269\,
J/k_B$.  All runs use $\mathbb{Z}_2$ symmetry
(Sec.~\ref{sec:z2-symmetry}).

\subsection{Setup}

\begin{itemize}
  \item \textbf{Base model (AR):} MADE with one hidden layer of 512 units
    ($\sim 1.05 \times 10^6$ parameters).
  \item \textbf{Flow (AR+flow):} 4 coupling layers with alternating
    checkerboard partitions; each mask network is an 8-hidden-layer ConvNet
    with 64 channels per layer and $3 \times 3$ kernels (receptive field
    $19 \times 19$ per layer; $\sim 1.04 \times 10^6$ flow parameters,
    matching the MADE parameter count).
  \item \textbf{Training:} REINFORCE for $\theta$, STE for $\phi$; Adam
    optimiser with $\alpha_\theta = \alpha_\phi = 10^{-3}$; batch size 500;
    10\,000 steps.
\end{itemize}
Exact reference values are computed via the Kaufman transfer-matrix
formula for the $32 \times 32$ periodic lattice.

\subsection{Results}

\begin{table}[h]
\centering
\caption{Variational free energy, energy, and entropy per site for the
  $32 \times 32$ Ising model at $T_c$, with $\mathbb{Z}_2$ symmetry.
  ``AR'' denotes the MADE-only baseline; ``AR+flow'' adds 4 discrete
  coupling layers.}
\label{tab:results}
\begin{tabular}{lccc}
\toprule
& $F_{\mathrm{var}} / N$ & $\langle E \rangle / N$
  & $S / N$ \\
\midrule
Exact (Kaufman)  & $-2.111$ & $-1.434$ & $0.298$ \\
AR only          & $-2.075$ & $-1.735$ & $0.150$ \\
AR + flow        & $-2.076$ & $-1.728$ & $0.153$ \\
\bottomrule
\end{tabular}
\end{table}

Two observations stand out:
\begin{enumerate}
  \item The gap to the exact solution, $\Delta F / N \approx 0.036$, is
    dominated by an \emph{entropy deficit}: $S/N = 0.15$ versus the exact
    $0.30$.  The model oversamples ordered, low-energy configurations
    ($\langle E \rangle / N = -1.73$ versus $-1.43$) at the expense of
    the exponentially many disordered configurations that contribute to
    the entropy at~$T_c$.
  \item Adding discrete coupling layers with comparable parameter count
    does \textbf{not} improve the variational free energy.
\end{enumerate}

\subsection{The entropy-preservation theorem for discrete flows}

The second observation has a simple structural explanation.

\begin{proposition}[Entropy preservation]
\label{prop:entropy}
Let $f_\phi : \{\pm 1\}^N \to \{\pm 1\}^N$ be any bijection, and let
$q(\bm{\sigma}) = p_\theta(f_\phi^{-1}(\bm{\sigma}))$.  Then
\begin{equation}
  H[q] = H[p_\theta].
  \label{eq:entropy-preservation}
\end{equation}
\end{proposition}
\begin{proof}
By change of summation variable $\bm{z} = f_\phi^{-1}(\bm{\sigma})$:
\begin{equation}
  H[q]
  = -\sum_{\bm{\sigma}} q(\bm{\sigma}) \ln q(\bm{\sigma})
  = -\sum_{\bm{z}} p_\theta(\bm{z}) \ln p_\theta(\bm{z})
  = H[p_\theta].  \qedhere
\end{equation}
\end{proof}

This is obvious in retrospect---a bijection on a finite set is a
permutation of probability masses, which cannot change the entropy---but
its consequences for the variational framework are important.  Decomposing
the variational free energy:
\begin{equation}
  \Fvar = \underbrace{\langle E(f_\phi(\bm{z})) \rangle_{p_\theta}}_{\text{flow can optimise}}
  \;-\; T \underbrace{H[p_\theta]}_{\text{flow cannot change}}.
  \label{eq:fvar-decomposition}
\end{equation}
The flow can only reduce $\Fvar$ by lowering the expected energy---it
rearranges which latent configurations map to which physical
configurations, routing high-probability $\bm{z}$'s to low-energy
$\bm{\sigma}$'s.  But it \emph{cannot increase the entropy} of the
variational distribution.

\subsection{REINFORCE mode collapse as the fundamental bottleneck}
\label{sec:mode-collapse}

The entropy deficit in Table~\ref{tab:results} is a symptom of
\emph{mode collapse} in the REINFORCE training of the base distribution.
The REINFORCE gradient estimator
\begin{equation}
  \nabla_\theta \Fvar
  \approx \frac{1}{M} \sum_{m=1}^{M}
  \big(R^{(m)} - b\big)\, \nabla_\theta \ln p_\theta(\bm{z}^{(m)})
\end{equation}
creates a positive feedback loop:
\begin{enumerate}
  \item Samples that happen to have low energy receive large negative
    advantage $\Rightarrow$ REINFORCE increases their probability.
  \item Higher probability for these configurations $\Rightarrow$ they
    are sampled even more frequently.
  \item Disordered configurations (which are exponentially more numerous
    but individually less probable) are undersampled $\Rightarrow$ their
    gradient contribution vanishes.
  \item The entropy collapses as the model concentrates on a
    shrinking set of ordered configurations.
\end{enumerate}
The entropy term $T \ln q$ in the reward provides a self-correcting
signal---as $q$ concentrates, $\ln q$ becomes less negative, worsening
the reward.  But with batch size $M = 500$ in a configuration space of
size $2^{1024}$, the gradient variance is enormous, and this
correction is overwhelmed by noise.

This failure mode is intrinsic to the reverse KL divergence optimised by
the variational free energy: $\KL(q \| p_{\mathrm{Boltz}})$ is
mode-seeking, preferring to concentrate on a single mode rather than
spread across all modes.  It is particularly severe at~$T_c$ where the
Boltzmann distribution is supported on an exponentially large set of
nearly degenerate domain configurations.

\subsection{Implications for the discrete flow framework}

The entropy-preservation theorem~\eqref{eq:entropy-preservation} reveals
a fundamental division of labour:
\begin{itemize}
  \item The \textbf{base distribution} $p_\theta$ controls the entropy
    budget---how much of configuration space the variational distribution
    covers.
  \item The \textbf{flow} $f_\phi$ controls the energy efficiency---given
    the entropy budget, it routes probability mass to the lowest-energy
    configurations.
\end{itemize}
The flow is beneficial when $p_\theta$ has high entropy but assigns
probability to energetically suboptimal configurations.  It is
\emph{useless} when $p_\theta$ has already mode-collapsed: rearranging
probability mass within a low-entropy distribution cannot recover the
missing entropy.

This motivates several directions for improving the framework:
\begin{enumerate}
  \item \textbf{Entropy regularisation}: explicitly penalise low entropy
    in $p_\theta$ during training, ensuring the base maintains sufficient
    diversity for the flow to work with.
  \item \textbf{Beta annealing}: start training at high temperature
    ($\beta \to 0$) where the entropy is naturally high, and gradually
    cool to the target $T_c$.  Our experiments with beta annealing
    ($\beta_{\mathrm{anneal}} = 0.998$) did yield higher entropy ($S/N =
    0.21$) but at the cost of a larger overall gap ($\Delta F/N = 0.11$),
    suggesting the annealing schedule requires careful tuning.
  \item \textbf{Larger batch sizes}: reducing REINFORCE variance to
    allow the entropy self-correction signal to compete with the
    energy-lowering signal.
  \item \textbf{Forward KL training}: replacing the mode-seeking reverse
    KL with the mode-covering forward KL, using MCMC samples from the
    Boltzmann distribution as training data (see
    Sec.~\ref{sec:forward-kl}).
\end{enumerate}

\subsection{Forward KL training: creating the right regime for the flow}
\label{sec:forward-kl}

The fundamental bottleneck identified above is that REINFORCE-based
reverse KL training collapses the entropy of the base distribution,
leaving the flow with nothing to improve.  Forward KL training offers a
qualitatively different approach that creates exactly the conditions
under which the discrete flow becomes beneficial.

\paragraph{Forward KL objective.}
Instead of minimising $\KL(q \| p_{\mathrm{Boltz}})$ (reverse KL), we
minimise the forward KL divergence
\begin{equation}
  \KL(p_{\mathrm{Boltz}} \| q)
  = \sum_{\bm{\sigma}} p_{\mathrm{Boltz}}(\bm{\sigma})
    \ln \frac{p_{\mathrm{Boltz}}(\bm{\sigma})}{q(\bm{\sigma})},
  \label{eq:forward-kl}
\end{equation}
which, up to the constant $-H[p_{\mathrm{Boltz}}]$, is equivalent to
minimising the negative log-likelihood (NLL) on samples drawn from the
Boltzmann distribution:
\begin{equation}
  \mathcal{L}_{\mathrm{NLL}}(\theta, \phi)
  = -\E_{\bm{\sigma} \sim p_{\mathrm{Boltz}}}\!\big[
    \ln q(\bm{\sigma})
  \big]
  = -\E_{\bm{\sigma} \sim p_{\mathrm{Boltz}}}\!\big[
    \ln p_\theta\!\big(f_\phi^{-1}(\bm{\sigma})\big)
  \big].
  \label{eq:nll-objective}
\end{equation}
Training samples $\{\bm{\sigma}^{(m)}\}$ are obtained from MCMC
(e.g.\ Wolff cluster algorithm) and the gradients are computed by
standard backpropagation through the exact log-probability
$\ln p_\theta(f_\phi^{-1}(\bm{\sigma}))$---no REINFORCE is needed.

\paragraph{Key advantages.}
\begin{enumerate}
  \item \textbf{Mode-covering behaviour.}  The forward KL
    $\KL(p_{\mathrm{Boltz}} \| q)$ is \emph{mode-covering}: it penalises
    $q$ wherever $p_{\mathrm{Boltz}}$ has support, forcing the model to
    spread probability across all relevant configurations rather than
    collapse onto a single mode.  This directly prevents the entropy
    collapse that plagues REINFORCE training.
  \item \textbf{Low-variance gradients.}  Maximum likelihood estimation
    on MCMC samples produces gradients with much lower variance than
    REINFORCE.  The gradient $\nabla_\theta \ln p_\theta(\bm{z})$ is a
    simple score function evaluated on a fixed sample---no reward signal,
    no baseline subtraction, no advantage estimation.
  \item \textbf{Correct entropy by construction.}  Since the training
    data comes from the Boltzmann distribution, which has entropy
    $S_{\mathrm{exact}}$, the model naturally learns to match this entropy
    level.  The resulting base distribution $p_\theta$ will have high
    entropy and assign probability to energetically suboptimal
    configurations.
\end{enumerate}

\paragraph{The ideal regime for the flow.}
Forward KL training creates exactly the conditions under which the
discrete flow becomes essential.  After training with NLL on MCMC
samples, the base distribution $p_\theta$ has:
\begin{itemize}
  \item \textbf{High entropy}: $H[p_\theta] \approx S_{\mathrm{exact}}$,
    covering the full support of the Boltzmann distribution.
  \item \textbf{Suboptimal energy}: the autoregressive factorisation
    cannot perfectly capture the spatial correlations in
    $p_{\mathrm{Boltz}}$, so the energy $\langle E(f_\phi(\bm{z}))
    \rangle$ is higher than optimal.
\end{itemize}
This is precisely the regime where the entropy-preservation
theorem~\eqref{eq:entropy-preservation} is not a limitation.  The flow's
role is to \emph{rearrange} the high-entropy probability mass, routing
it toward lower-energy configurations without compressing it---lowering
$\langle E \rangle$ while preserving the entropy budget.

\paragraph{Empirical evidence: Tran et al.\ on the Potts model.}
The prediction that forward KL training enables the flow to improve over
the autoregressive baseline is supported by the experiments of Tran et
al.~\cite{tran2019}, who trained discrete autoregressive flows on the
2D $q$-state Potts model using maximum likelihood on Metropolis--Hastings
samples.  On small lattices ($3 \times 3$ and $4 \times 4$, with
$q = 3, 4, 5$), the discrete flow consistently matched or improved over
the autoregressive baseline in terms of negative log-likelihood (NLL).
The gains were largest at weak coupling ($J = 0.1$), where the base
distribution has high entropy but poor spatial correlations---precisely
the regime identified above.  At the strongest coupling ($J = 0.5$,
$3 \times 3$, $q = 3$), the flow provided no improvement, consistent
with the base already capturing the correlations of this small, strongly
ordered system.

These results provide a useful contrast with our reverse KL experiments
in Table~\ref{tab:results}.  The key difference is the training
objective: Tran et al.\ train on MCMC samples (forward KL), which
preserves entropy and creates the conditions for the flow to help.  Our
REINFORCE-based training (reverse KL) collapses the entropy first,
leaving the flow with nothing to improve.  However, the Tran et al.\
experiments have significant limitations: the systems are tiny (9--16
spins), the flow network is a lookup table that cannot scale, no
comparison to exact partition functions is provided, and---crucially---no
variational free-energy bound is produced.  The hybrid approach proposed
below addresses these gaps.

\paragraph{Hybrid training: Forward KL $\to$ Reverse KL.}
A practical training protocol combines both objectives:
\begin{enumerate}
  \item \textbf{Phase 1 (Forward KL):} Train both $\theta$ and $\phi$ on
    MCMC samples via the NLL objective~\eqref{eq:nll-objective}.  This
    establishes the correct entropy level and teaches the flow to improve
    spatial correlations.
  \item \textbf{Phase 2 (Reverse KL):} Fine-tune by minimising the
    variational free energy $\Fvar$ via REINFORCE (for $\theta$) and STE
    (for $\phi$).  Starting from the high-entropy initialisation of
    Phase~1, the entropy self-correction signal in the REINFORCE reward
    is now strong enough to prevent collapse, and the variational bound
    is tightened.
\end{enumerate}
Phase~1 solves the cold-start problem: instead of REINFORCE exploring a
$2^N$-dimensional space from scratch, it begins with a model that
already assigns reasonable probability to the correct configurations.
Phase~2 recovers the rigorous variational bound, which is not guaranteed
by the forward KL alone.

\paragraph{Applicability and limitations.}
Forward KL training requires an efficient MCMC sampler for the target
distribution.  For the 2D ferromagnetic Ising model, the Wolff cluster
algorithm has a dynamic critical exponent $z \approx 0.25$, making MCMC
nearly free even at $T_c$.  More generally, forward KL is applicable
whenever:
\begin{itemize}
  \item Local or cluster MCMC updates can decorrelate configurations in
    polynomial time (unfrustrated spin systems, some lattice field
    theories).
  \item Parallel tempering or other enhanced sampling methods provide
    sufficiently diverse samples.
\end{itemize}
For \emph{frustrated} or \emph{glassy} systems---where MCMC itself
suffers from exponential autocorrelation times---forward KL training is
not viable, and the pure reverse KL approach (with improvements such as
entropy regularisation and annealing) remains the only option.  This
creates an interesting dichotomy: the discrete flow framework is most
straightforwardly useful (via forward KL) for systems where MCMC works
but the variational ansatz benefits from a learned change of variables,
and most needed but hardest to train (via reverse KL) for systems where
MCMC fails.

%======================================================================
\section{Beyond bijective flows: relaxing the bijectivity constraint}
\label{sec:beyond-bijective}
%======================================================================

The entropy-preservation theorem (Proposition~\ref{prop:entropy}) identifies
a fundamental limitation of bijective discrete flows: the flow cannot
increase the entropy of the variational distribution, so the entire entropy
budget must come from the base $p_\theta$.  A natural question is whether
the bijectivity constraint can be relaxed while maintaining the rigorous
variational bound on $\Ftrue$.

The standard relaxations from the latent-variable literature fail:
the ELBO provides a \emph{lower} bound on $\ln q(\bm{\sigma})$, which,
when substituted into~\eqref{eq:var-free-energy}, yields a quantity
\emph{below} $\Fvar$---not a valid upper bound on $\Ftrue$.  The joint
bound (Sec.~\ref{sec:rigour}) is rigorous but loose by an extensive
amount.  We describe three approaches that succeed, each with different
tradeoffs.

\subsection{Markov-corrected variational ansatz}
\label{sec:markov-corrected}

The simplest extension appends a \emph{fixed} Markov transition kernel to
the autoregressive base, with no bijective flow.  The generative process is
\begin{equation}
  \bm{z} \sim p_\theta(\bm{z}),
  \qquad
  \bm{\sigma} \sim T(\cdot \mid \bm{z}),
  \label{eq:mc-generative}
\end{equation}
where $T$ is a fixed, analytically known transition kernel (e.g., a Gibbs
sweep targeting the Boltzmann distribution).  The variational distribution
is the marginal
\begin{equation}
  q(\bm{\sigma})
  = \sum_{\bm{z}} p_\theta(\bm{z})\, T(\bm{\sigma} \mid \bm{z}),
  \label{eq:mc-marginal}
\end{equation}
which is intractable---evaluating $\ln q(\bm{\sigma})$ requires summing
over all $\bm{z}$.  Nevertheless, a rigorous variational bound can be
obtained without evaluating $\ln q$.

\paragraph{Derivation of the bound.}
Define a \emph{reverse kernel} $\tilde{T}(\bm{z} \mid \bm{\sigma})$---an
auxiliary distribution that approximates the posterior over $\bm{z}$ given
$\bm{\sigma}$.  Consider the joint distribution of the forward process,
$P_{\mathrm{fwd}}(\bm{z}, \bm{\sigma}) = p_\theta(\bm{z})\,
T(\bm{\sigma} \mid \bm{z})$, and the ``target joint''
$P_{\mathrm{target}}(\bm{z}, \bm{\sigma}) =
p_{\mathrm{Boltz}}(\bm{\sigma})\, \tilde{T}(\bm{z} \mid \bm{\sigma})$.
Since $\KL(P_{\mathrm{fwd}} \| P_{\mathrm{target}}) \geq 0$:
\begin{align}
  0 &\leq \E_{P_{\mathrm{fwd}}}\!\Big[
    \ln \frac{p_\theta(\bm{z})\, T(\bm{\sigma}|\bm{z})}{
      p_{\mathrm{Boltz}}(\bm{\sigma})\, \tilde{T}(\bm{z}|\bm{\sigma})}
  \Big] \nonumber\\
  &= \E\!\big[\ln p_\theta(\bm{z})
    + \ln T(\bm{\sigma}|\bm{z})
    - \ln \tilde{T}(\bm{z}|\bm{\sigma})
    + \beta E(\bm{\sigma}) + \ln Z
  \big],
\end{align}
which rearranges to
\begin{equation}
  \boxed{%
    \Ftrue \leq \E\!\Big[
      E(\bm{\sigma}) + T \ln p_\theta(\bm{z})
      + T \ln \frac{T(\bm{\sigma}|\bm{z})}{\tilde{T}(\bm{z}|\bm{\sigma})}
    \Big],
  }
  \label{eq:mc-bound}
\end{equation}
where the expectation is over $\bm{z} \sim p_\theta$,
$\bm{\sigma} \sim T(\cdot|\bm{z})$.  This is a \emph{rigorous upper bound}
on $\Ftrue$ for \emph{any} choice of $T$ and $\tilde{T}$.

\paragraph{Structure of the bound.}
Equation~\eqref{eq:mc-bound} decomposes into three interpretable terms:
\begin{equation}
  \tilde{F}
  = \underbrace{\E[E(\bm{\sigma})]}_{\text{energy}}
  \;\underbrace{- T\, H[p_\theta]}_{\text{base entropy}}
  \;\underbrace{+ T \E\!\Big[\ln \frac{T(\bm{\sigma}|\bm{z})}{
    \tilde{T}(\bm{z}|\bm{\sigma})}\Big]}_{\text{Markov correction}}.
  \label{eq:mc-three-terms}
\end{equation}
The first two terms are the standard variational free energy of $p_\theta$,
as if the configurations were $\bm{z}$ rather than $\bm{\sigma}$.  The
third term is the cost of the stochastic correction---the average
log-ratio of forward to reverse transitions.

\paragraph{Gap analysis.}
The gap between~\eqref{eq:mc-bound} and the true variational free energy
$\Fvar[q]$ of the marginal~\eqref{eq:mc-marginal} is
\begin{equation}
  \tilde{F} - \Fvar[q]
  = T \E_{q(\bm{\sigma})}\!\big[
    \KL\!\big(p(\bm{z}|\bm{\sigma}) \,\big\|\,
    \tilde{T}(\bm{z}|\bm{\sigma})\big)
  \big]
  \geq 0,
  \label{eq:mc-gap}
\end{equation}
where $p(\bm{z}|\bm{\sigma}) = p_\theta(\bm{z})\,T(\bm{\sigma}|\bm{z}) /
q(\bm{\sigma})$ is the true posterior.  The bound is tight when $\tilde{T}$
matches the posterior exactly.

\paragraph{Specialisation to Gibbs sweeps.}
For the Ising model, the natural choice is a systematic-scan Gibbs sweep
targeting the Boltzmann distribution.  Here $\bm{z}$ is the
\emph{starting} (old) configuration produced by the AR base, and
$\bm{\sigma}$ is the \emph{output} (new) configuration on which the energy
is evaluated.  The sweep updates spins sequentially in order
$i = 1, \ldots, N$: at step~$i$, sites $j < i$ already hold their new
values $\sigma_j$, while sites $j > i$ still hold their old values $z_j$.
The transition probability factorises as
\begin{equation}
  T(\bm{\sigma} \mid \bm{z})
  = \prod_{i=1}^{N}
    p_{\mathrm{Boltz}}(\sigma_i \mid \sigma_{<i},\, z_{>i}),
  \label{eq:gibbs-forward}
\end{equation}
where each factor is the single-site Gibbs conditional
\begin{equation}
  p_{\mathrm{Boltz}}(\sigma_i \mid \text{neighbours})
  = \frac{e^{\beta \sigma_i h_i}}{2\cosh \beta h_i},
  \qquad
  h_i = J \sum_{j \sim i} s_j,
  \qquad
  s_j =
  \begin{cases}
    \sigma_j & \text{if } j < i \;\text{(already updated)},\\
    z_j      & \text{if } j > i \;\text{(not yet updated)}.
  \end{cases}
  \label{eq:gibbs-conditional}
\end{equation}
On a 2D lattice, site~$i$ typically has neighbours with both lower and
higher indices, so the local field $h_i$ genuinely mixes old ($z$) and new
($\sigma$) spin values.  Each factor is analytically known, so
$\ln T(\bm{\sigma}|\bm{z})$ is a sum of $N$ tractable terms.

For the reverse kernel, a Gibbs sweep in the \emph{reverse} order
$i = N, N-1, \ldots, 1$ is a natural, parameter-free choice:
\begin{equation}
  \tilde{T}(\bm{z} \mid \bm{\sigma})
  = \prod_{i=N}^{1}
    p_{\mathrm{Boltz}}(z_i \mid z_{>i},\, \sigma_{<i}).
  \label{eq:gibbs-reverse}
\end{equation}
Because the forward and reverse sweeps visit sites in opposite order, they
see different mixtures of old and new spins at each step, so
$\ln T(\bm{\sigma}|\bm{z}) \neq \ln \tilde{T}(\bm{z}|\bm{\sigma})$ in
general.  Both $\ln T$ and $\ln \tilde{T}$ are sums of $N$ local Boltzmann
conditionals---no neural networks, no learning, no STE.

\paragraph{The telescoping problem.}
Despite the apparent asymmetry between forward and reverse sweeps, the
Markov correction in~\eqref{eq:mc-bound} \emph{trivialises} when both
kernels target the Boltzmann distribution at the same temperature.  At
step~$i$, both the forward and reverse sweeps condition on the same
background $(\sigma_{<i},\, z_{>i})$, so the local field $h_i$ is
identical.  The per-site log-ratio is therefore
\begin{equation}
  \ln p_{\mathrm{Boltz}}(\sigma_i \mid \sigma_{<i}, z_{>i})
  - \ln p_{\mathrm{Boltz}}(z_i \mid \sigma_{<i}, z_{>i})
  = \beta(\sigma_i - z_i)\, h_i.
  \label{eq:per-site-ratio}
\end{equation}
Summing over all sites, this is a telescoping sum of single-spin energy
changes.  Define the sequence of partially updated configurations
$\bm{c}^{(i)} = (\sigma_1, \ldots, \sigma_i, z_{i+1}, \ldots, z_N)$, so
that $\bm{c}^{(0)} = \bm{z}$ and $\bm{c}^{(N)} = \bm{\sigma}$.  The
energy change at step~$i$ is
\begin{equation}
  E(\bm{c}^{(i)}) - E(\bm{c}^{(i-1)})
  = -(\sigma_i - z_i)\, h_i,
\end{equation}
so the total log-ratio telescopes:
\begin{equation}
  \ln \frac{T(\bm{\sigma}|\bm{z})}{\tilde{T}(\bm{z}|\bm{\sigma})}
  = \beta \sum_{i=1}^{N} (\sigma_i - z_i)\, h_i
  = -\beta \sum_{i=1}^{N} \big[E(\bm{c}^{(i)}) - E(\bm{c}^{(i-1)})\big]
  = \beta\big[E(\bm{z}) - E(\bm{\sigma})\big].
  \label{eq:telescoping}
\end{equation}
Substituting into the bound~\eqref{eq:mc-bound}:
\begin{equation}
  \tilde{F}
  = \E\!\big[E(\bm{\sigma}) + T \ln p_\theta(\bm{z})
    + E(\bm{z}) - E(\bm{\sigma})\big]
  = \E\!\big[E(\bm{z}) + T \ln p_\theta(\bm{z})\big]
  = \Fvar[p_\theta].
  \label{eq:trivialisation}
\end{equation}
\emph{The Gibbs sweep drops out entirely.}  The bound reduces to the
variational free energy of the base distribution, as if no MCMC step had
been applied.  This is not a coincidence: any transition kernel $T$ that
satisfies detailed balance with respect to $p_{\mathrm{Boltz}}$, paired
with the natural reverse $\tilde{T} = T$ or any $\tilde{T}$ whose
log-ratio with $T$ equals $\beta \Delta E$, produces the same
cancellation.  In particular, the Wolff cluster algorithm, random-scan
Gibbs, and Swendsen--Wang all trivialise the bound for the same reason.

The physical interpretation is clear: a single MCMC step at the target
temperature is ``too well matched'' to the Boltzmann distribution.  The
importance weight exactly compensates for any energy change the kernel
produces, yielding zero net improvement.

\paragraph{The invisible improvement.}
The trivialisation of the bound does \emph{not} mean the Gibbs sweep is
useless---it means the bound cannot see the improvement.  The variational
distribution after the sweep,
\begin{equation}
  q(\bm{\sigma})
  = \sum_{\bm{z}} p_\theta(\bm{z})\, T(\bm{\sigma} \mid \bm{z}),
\end{equation}
is genuinely different from $p_\theta$ and provably better.  Since $T$
leaves $p_{\mathrm{Boltz}}$ invariant, the data processing inequality for
Markov chains guarantees contraction of the KL divergence:
\begin{equation}
  \KL(q \| p_{\mathrm{Boltz}})
  \leq \KL(p_\theta \| p_{\mathrm{Boltz}}),
  \label{eq:kl-contraction}
\end{equation}
which is equivalent to
\begin{equation}
  \Fvar[q] \leq \Fvar[p_\theta].
  \label{eq:fvar-contraction}
\end{equation}
The Gibbs sweep is a \emph{provable improvement operator} on the
variational distribution: it increases entropy (by convolving $p_\theta$
with the stochastic kernel) and adjusts the energy, with the net effect
always reducing the variational free energy.

After $k$ sweeps, the contraction compounds:
\begin{equation}
  \KL(q_k \| p_{\mathrm{Boltz}})
  \leq (1 - \gamma)^k\, \KL(p_\theta \| p_{\mathrm{Boltz}}),
\end{equation}
where $\gamma$ is the spectral gap of the transition matrix.  Near $T_c$,
$\gamma \sim L^{-z}$ with dynamic critical exponent $z \approx 2$ for
single-site Gibbs and $z \approx 0.25$ for Wolff, so the contraction per
sweep is slow for Gibbs but fast for Wolff.

The problem is that $\Fvar[q]$ is \emph{not computable}: evaluating
$\ln q(\bm{\sigma})$ requires the intractable sum over all $\bm{z}$.  The
hierarchy of bounds is therefore
\begin{equation}
  \Ftrue
  \;\leq\; \Fvar[q]
  \;\leq\; \Fvar[p_\theta]
  \;=\; \tilde{F},
  \label{eq:bound-hierarchy}
\end{equation}
where the gap $\Fvar[p_\theta] - \Fvar[q] > 0$ is a real improvement that
the evaluable bound $\tilde{F}$ cannot capture.  The role of
$\beta$-annealing, introduced next, is precisely to make this hidden
improvement visible.

\paragraph{Resolution: $\beta$-annealing.}
To obtain a non-trivial bound, the transition kernels must target
distributions at \emph{intermediate} temperatures, not the physical
temperature.  This is the standard annealed importance sampling (AIS)
construction~\cite{neal2001ais}, here combined with a learned base.

Define a temperature schedule $0 < \beta_1 < \beta_2 < \cdots <
\beta_K = \beta$ and intermediate (unnormalised) distributions
$\tilde{\pi}_k(\bm{x}) = e^{-\beta_k E(\bm{x})}$.  Set
$\tilde{\pi}_0(\bm{x}) = p_\theta(\bm{x})$ (the learned base, which is
normalised).  The generative process is:
\begin{enumerate}
  \item Sample $\bm{x}^{(0)} \sim p_\theta$.
  \item For $k = 1, \ldots, K$: apply an MCMC kernel $T_k$ that leaves
    $\pi_k \propto \tilde{\pi}_k$ invariant, producing
    $\bm{x}^{(k)} \sim T_k(\cdot \mid \bm{x}^{(k-1)})$.
  \item Output $\bm{\sigma} = \bm{x}^{(K)}$.
\end{enumerate}
The AIS log-weight is
\begin{equation}
  \ln W = \sum_{k=1}^{K} \ln
    \frac{\tilde{\pi}_k(\bm{x}^{(k-1)})}{\tilde{\pi}_{k-1}(\bm{x}^{(k-1)})}
  = \ln \frac{\tilde{\pi}_1(\bm{x}^{(0)})}{p_\theta(\bm{x}^{(0)})}
  + \sum_{k=2}^{K}
    \big[-(\beta_k - \beta_{k-1})\, E(\bm{x}^{(k-1)})\big].
  \label{eq:ais-weight}
\end{equation}
The \emph{MCMC kernels do not appear in the weight}---only the energies at
intermediate configurations and the base log-probability.  This means
\emph{any} mixing kernel can be used at each step: Gibbs sweeps, Wolff
cluster updates, or Swendsen--Wang, without needing to evaluate their
(potentially intractable) transition probabilities.

By Jensen's inequality applied to the identity
$\E[W] = Z / Z_0$~\cite{neal2001ais,jarzynski1997}, the bound is
\begin{equation}
  \boxed{%
    \Ftrue \leq -T\, \E[\ln W]
    = \E\!\Big[
      -T \ln \frac{\tilde{\pi}_1(\bm{x}^{(0)})}{p_\theta(\bm{x}^{(0)})}
      + \sum_{k=2}^{K} T(\beta_k - \beta_{k-1})\, E(\bm{x}^{(k-1)})
    \Big].
  }
  \label{eq:ais-bound}
\end{equation}
This is a rigorous upper bound on $\Ftrue$ for any number of annealing
steps $K$, any temperature schedule, and any choice of MCMC kernels.
Crucially, the AIS bound sits between the invisible improvement and the
trivialised bound, refining the hierarchy~\eqref{eq:bound-hierarchy}:
\begin{equation}
  \Ftrue
  \;\leq\; \Fvar[q_K]
  \;\leq\; \tilde{F}_{\mathrm{AIS}}(K)
  \;\leq\; \cdots
  \;\leq\; \tilde{F}_{\mathrm{AIS}}(1)
  \;\leq\; \Fvar[p_\theta].
  \label{eq:ais-hierarchy}
\end{equation}
The AIS bound makes the hidden improvement of the MCMC
steps~\eqref{eq:fvar-contraction} partially visible, with the gap
$\tilde{F}_{\mathrm{AIS}}(K) - \Fvar[q_K]$ controlled by the variance of
$\ln W$.

\paragraph{Role of the learned base.}
In standard AIS, the base distribution is simple (e.g., uniform, with
$\tilde{\pi}_0 = 1$ and $Z_0 = 2^N$), requiring many temperature steps to
bridge the gap to $p_{\mathrm{Boltz}}$.  Replacing the uniform base with a
learned autoregressive model $p_\theta$ that already approximates
$p_{\mathrm{Boltz}}$ dramatically reduces the number of annealing steps
needed.  If $p_\theta$ is perfect, $K = 0$ suffices and the bound reduces
to $\Fvar[p_\theta] = \Ftrue$.  In practice, a moderate $K$ (a few Wolff
sweeps across a short temperature range) suffices to correct the entropy
deficit that $p_\theta$ alone cannot resolve.

\paragraph{Choice of MCMC kernel.}
Since the kernel does not appear in the weight, the only criterion is
\emph{mixing efficiency}.  Near $T_c$, the Wolff cluster algorithm has
dynamic critical exponent $z \approx 0.25$, compared to $z \approx 2$ for
single-site Gibbs.  This makes Wolff the natural choice: each annealing
step thermalises the system efficiently, keeping the variance of $\ln W$
low with fewer steps.

\paragraph{Training.}
The bound~\eqref{eq:ais-bound} is minimised with respect to $\theta$ alone.
Since the sampling distribution depends on $\theta$ through $p_\theta$
(and indirectly through the MCMC chain), the gradient is a REINFORCE
estimator:
\begin{equation}
  \nabla_\theta \tilde{F}
  = \E\!\Big[
    \big(\tilde{F}_{\mathrm{local}} - b\big)\,
    \nabla_\theta \ln p_\theta(\bm{x}^{(0)})
  \Big],
  \label{eq:ais-reinforce}
\end{equation}
where $\tilde{F}_{\mathrm{local}} = -T \ln W$ is the per-sample bound
estimand and $b$ is a learned baseline.  This is identical in form
to~\eqref{eq:reinforce}.  The MCMC kernels are not differentiated through;
they are treated as fixed, non-trainable layers.

\paragraph{Entropy increase.}
The variational distribution $q(\bm{\sigma})$ is the marginal of the AIS
chain, which is a convolution of $p_\theta$ with $K$ MCMC kernels at
progressively higher $\beta$.  Each kernel broadens the distribution, so
\begin{equation}
  H[q] \geq H[p_\theta],
\end{equation}
with the gap growing with $K$.  The $\beta$-annealing injects entropy
\emph{gradually}: at low $\beta_k$ (high temperature), the MCMC kernels
are highly stochastic and inject substantial entropy; at high $\beta_k$
(near the physical temperature), they make only small, targeted
corrections.  This graduated approach avoids the all-or-nothing character
of a single MCMC step at the target temperature.

\paragraph{Interpolation between variational and MCMC.}
The construction provides a smooth interpolation:
\begin{itemize}
  \item $K = 0$: pure variational ansatz, $\tilde{F} = \Fvar[p_\theta]$.
  \item $K$ moderate: learned base + short AIS correction.
  \item $K \to \infty$ with mixing: $\tilde{F} \to \Ftrue$ regardless of
    $p_\theta$.
\end{itemize}
The learned base reduces the number of annealing steps needed for a given
accuracy, while the AIS correction patches the entropy deficit that the
base alone cannot resolve.  This is the key advantage over both pure
variational methods (which are entropy-limited) and pure AIS (which
requires many steps from a simple base).

\paragraph{Effect on training dynamics.}
A crucial consequence of the $\beta$-annealing is that it changes the
\emph{optimisation landscape} for $\theta$.  The AIS objective
$\tilde{F}_{\mathrm{AIS}}(\theta) = \E[-T \ln W]$ is a different function
of $\theta$ than $\Fvar[p_\theta]$, and minimising it yields a different
optimal $\theta^*$.

In the pure variational setting ($K = 0$), the REINFORCE reward for a
sample $\bm{z}$ is $r(\bm{z}) = E(\bm{z}) + T \ln p_\theta(\bm{z})$.
A sample that happens to land on a low-energy configuration receives a
large negative reward, gets upweighted, and the positive feedback loop of
Sec.~\ref{sec:mode-collapse} drives mode collapse.  With $\beta$-annealing,
the reward becomes
\begin{equation}
  r_{\mathrm{AIS}}(\bm{z})
  = -T \ln W
  = -T \ln p_\theta(\bm{z})
  + \sum_{k} T \Delta\beta_k\, E(\bm{x}^{(k-1)}),
  \label{eq:ais-reward}
\end{equation}
which depends on the \emph{entire annealing trajectory}, not just
$\bm{z}$.  A ``bad'' starting configuration (high energy, wrong structure)
can still produce a good trajectory if the MCMC steps correct it.  The
reward is smoothed by the chain.

This changes what $p_\theta$ needs to learn.  In the pure variational
setting, $p_\theta$ must simultaneously capture both the energy and the
entropy of $p_{\mathrm{Boltz}}$---a task at which REINFORCE fails because
the mode-seeking reverse KL sacrifices entropy.  In the AIS setting,
$p_\theta$ need only provide a good \emph{proposal distribution} for the
annealing chain: the MCMC steps handle the energy correction.  A
high-entropy $p_\theta$ that roughly captures the correlation structure is
rewarded, because diverse starting points lead to better chain coverage.

The mechanism is variance reduction.  The per-sample reward variance
$\mathrm{Var}[\ln W]$ decreases with $K$: more annealing steps thermalise
bad samples, reducing the spread of rewards.  Lower variance produces a
more uniform gradient signal across samples, preventing $p_\theta$ from
over-concentrating on a few low-energy modes.  The number of annealing
steps $K$ thus serves as a tunable knob that trades compute for training
stability.

In summary, the $\beta$-annealing breaks the REINFORCE mode collapse
feedback loop by changing the role of $p_\theta$ from ``approximate
$p_{\mathrm{Boltz}}$ alone'' to ``provide a good starting point for the
annealing chain''---a fundamentally easier optimisation problem.

\paragraph{Alternative base: tensor train / matrix product state.}
The preceding discussion raises the question of what parameterisation is
best suited for a high-entropy base distribution.  A natural candidate is
the \emph{tensor train} (TT) decomposition, known in physics as a
\emph{matrix product state} (MPS).  For $N$ binary spins, the TT
represents the distribution as
\begin{equation}
  p(\bm{\sigma})
  = \mathrm{Tr}\!\big[
    A_1(\sigma_1)\, A_2(\sigma_2) \cdots A_N(\sigma_N)
  \big],
  \label{eq:tt-distribution}
\end{equation}
where each $A_i(\sigma_i)$ is a $D \times D$ matrix and $D$ is the
\emph{bond dimension}.  Like the autoregressive network, the TT provides
exact log-probabilities (computable in $O(ND^2)$ by matrix multiplication)
and exact sampling (via a left-to-right sweep using the chain rule).  In
fact, the TT \emph{is} an autoregressive model: the conditional
$p(\sigma_i \mid \sigma_{<i})$ is determined by the left environment
contracted with $A_i$ and a right normalisation factor.

The key advantage of the TT as a base for the $\beta$-annealed framework
is threefold:
\begin{enumerate}
  \item \textbf{Implicit entropy regularisation.}
    The mutual information across any bond of the TT is bounded by
    $\ln D$.  A mode-collapsed distribution---concentrated on a few
    configurations---requires high mutual information across cuts, which
    demands large $D$.  At moderate $D$, the TT \emph{cannot}
    mode-collapse: the bond dimension imposes a structural upper bound on
    how peaked the distribution can be.  In contrast, an autoregressive
    neural network is a universal approximator that can represent
    arbitrarily concentrated distributions, with no built-in safeguard
    against mode collapse.

  \item \textbf{DMRG training.}
    The TT admits optimisation by the density matrix renormalisation group
    (DMRG), which sweeps through the chain optimising one tensor core at a
    time via a local least-squares or eigenvalue problem.  This is
    \emph{not} REINFORCE: there is no high-variance score-function
    estimator, no positive feedback loop, and no mode collapse.  The
    convergence is stable and well understood.  (If the TT were trained
    with REINFORCE, it would mode-collapse just like an AR network---the
    advantage is that the TT admits a better optimiser.)

  \item \textbf{Bond dimension controls the effective temperature.}
    At $D = 1$, the TT is a product distribution (independent spins),
    which has maximum entropy $N \ln 2$.  As $D$ increases, the TT can
    represent progressively longer-range correlations, up to a correlation
    length $\xi_{\mathrm{MPS}} \sim \ln D$ (for a 2D system mapped to 1D).
    Choosing $D$ such that $\xi_{\mathrm{MPS}} \approx \xi(T_{\mathrm{eff}})$
    gives a base distribution that naturally approximates the Boltzmann
    distribution at an effective temperature $T_{\mathrm{eff}} > T_c$.
    The $\beta$-annealing chain then cools from $T_{\mathrm{eff}}$ to the
    physical temperature, building the long-range correlations that $D$
    cannot capture.
\end{enumerate}

The main limitation is the 1D structure: for a 2D $L \times L$ lattice
mapped to 1D via a snake ordering, the entanglement entropy across a cut
scales as $O(L)$, requiring $D \sim e^{O(L)}$ for exact representation at
$T_c$.  At moderate $D$, the TT captures only short-range correlations---
but this is precisely the regime where it serves as a high-temperature base
for the AIS chain.  The bond dimension $D$ plays the same role as the
effective temperature $T_{\mathrm{eff}}$: it determines how much of the
physics $p_\theta$ captures versus how much the annealing chain must supply.

\subsection{Adding bijective layers: stochastic discrete normalizing flows}
\label{sec:stochastic-nf}

The $\beta$-annealed ansatz of Sec.~\ref{sec:markov-corrected} can be
strengthened by inserting bijective coupling layers between the MCMC steps.
Adapting the continuous stochastic normalizing flows of
Wu et al.~\cite{wu2020snf}, the generative process becomes
\begin{equation}
  \bm{z} \sim p_\theta(\bm{z}),
  \qquad
  \bm{\sigma}' = f_\phi(\bm{z}),
  \qquad
  \bm{\sigma}' \xrightarrow{\text{AIS}} \bm{\sigma},
  \label{eq:snf-generative}
\end{equation}
where the AIS chain starts from the flow output $\bm{\sigma}'$ rather than
directly from $\bm{z}$.  The bijective flow contributes no additional terms
to the AIS log-weight---its ``Jacobian'' is~1 on a finite set---but it
rearranges probability mass \emph{deterministically}, lowering
$\E[E(\bm{\sigma}')]$ at zero variance cost.

More generally, bijective coupling layers and $\beta$-annealed MCMC
steps can be freely interleaved:
\begin{equation}
  \bm{z}
  \;\xrightarrow{f_\phi}\;
  \bm{x}^{(0)}
  \;\xrightarrow{T_1 \text{ at } \beta_1}\;
  \bm{x}^{(1)}
  \;\xrightarrow{g^{(1)}_\phi}\;
  \bm{x}^{(1)}
  \;\xrightarrow{T_2 \text{ at } \beta_2}\;
  \cdots
  \;\xrightarrow{T_K \text{ at } \beta}\;
  \bm{\sigma},
\end{equation}
with the AIS log-weight~\eqref{eq:ais-weight} receiving contributions only
from the $\beta$-annealing ratios; the bijective layers are transparent.

The three components play distinct roles in the bound:
\begin{equation}
  \tilde{F}
  = \underbrace{\E[E(\bm{\sigma})]}_{\substack{
    \text{flow lowers,}\\
    \text{AIS corrects}}}
  \;\underbrace{- T\, H[p_\theta]}_{\substack{
    \text{AR base}\\
    \text{sets entropy}}}
  \;\underbrace{+ T\, \E[\text{AIS overhead}]}_{\substack{
    \text{annealing injects entropy}\\
    \text{at variance cost}}}.
  \label{eq:snf-three-terms}
\end{equation}
The bijective flow reduces the first term without affecting the other two.
The $\beta$-annealing reduces the gap between the base entropy and the
target entropy, at the cost of increasing the third term.  The optimal
architecture uses the flow to handle what it can (energy rearrangement)
and the annealing to handle what only it can (entropy injection).

\paragraph{Training.}
The gradients decompose cleanly across the parameter groups:
\begin{itemize}
  \item $\nabla_\theta$ (AR base): REINFORCE, as
    in~\eqref{eq:ais-reinforce}.
  \item $\nabla_\phi$ (bijective flow): STE through the coupling layers, as
    in~\eqref{eq:grad-phi}.  The flow gradient does not involve REINFORCE
    and does not depend on the MCMC kernels.
\end{itemize}
Only $\theta$ and $\phi$ are trained---the MCMC kernels and $\beta$-schedule
are fixed, non-trainable components that inject entropy for free.

\subsection{$k$-to-1 surjections via symmetry}
\label{sec:symmetry-surjection}

A complementary approach exploits the known symmetries of the target
distribution to construct surjective maps with small, tractable fibers.

\paragraph{General construction.}
Let $G$ be a finite symmetry group of the Hamiltonian:
$E(g \cdot \bm{\sigma}) = E(\bm{\sigma})$ for all $g \in G$, so that
$p_{\mathrm{Boltz}}(g \cdot \bm{\sigma}) =
p_{\mathrm{Boltz}}(\bm{\sigma})$.
Given a bare autoregressive model $p_\theta^{\mathrm{AR}}(\bm{z})$ and a
$G$-equivariant flow $f_\phi$, define the $G$-symmetrised distribution
\begin{equation}
  q(\bm{\sigma})
  = \frac{1}{|G|} \sum_{g \in G}
    p_\theta^{\mathrm{AR}}\!\big(f_\phi^{-1}(g \cdot \bm{\sigma})\big).
  \label{eq:G-symmetrised}
\end{equation}
This is a $|G|$-to-1 surjection from the ``labeled'' space (where the
orientation of $\bm{\sigma}$ relative to the lattice matters) to the
``unlabeled'' quotient.  The log-probability is evaluated exactly via
\begin{equation}
  \ln q(\bm{\sigma})
  = \mathrm{logsumexp}_{g \in G}\!\big(
    \ln p_\theta^{\mathrm{AR}}(f_\phi^{-1}(g \cdot \bm{\sigma}))
  \big) - \ln |G|.
  \label{eq:G-logprob}
\end{equation}
Since each term in the logsumexp is an exact autoregressive log-probability,
$\ln q(\bm{\sigma})$ is exact, and the variational bound
$\Fvar[q] \geq \Ftrue$ is rigorous.

\paragraph{The $\mathbb{Z}_2$ case.}
The symmetrisation~\eqref{eq:z2-base} already used for the spin-flip
symmetry is the special case $G = \mathbb{Z}_2 = \{e, -\mathbb{1}\}$ with
$|G| = 2$.  The logsumexp is over 2 terms and is trivially cheap.

\paragraph{Lattice symmetries.}
For the square-lattice Ising model with periodic boundary conditions, the
full space group is
\begin{equation}
  G = \underbrace{\mathbb{Z}_L \times \mathbb{Z}_L}_{\text{translations}}
  \;\rtimes\; \underbrace{D_4}_{\text{point group}}
  \;\times\; \underbrace{\mathbb{Z}_2}_{\text{spin flip}},
\end{equation}
with $|G| = L^2 \times 8 \times 2 = 16 L^2$.  For an $L = 32$ lattice,
$|G| = 16{,}384$.  Evaluating~\eqref{eq:G-logprob} requires $|G|$ forward
passes of the inverse flow and the autoregressive network, which is
expensive but embarrassingly parallel.  In practice, one may use a subgroup
(e.g., translations only, $|G| = L^2 = 1024$) to balance cost and benefit.

\paragraph{Entropy gain.}
Unlike the bijective flow alone, the $G$-symmetrised
distribution~\eqref{eq:G-symmetrised} has entropy
\begin{equation}
  H[q] = H[p_\theta^{\mathrm{AR}}]
  + \underbrace{\Delta H_G}_{\geq\, 0,\;\leq\, \ln|G|},
\end{equation}
where $\Delta H_G$ is the entropy gained from symmetrisation.  The gain is
maximal ($\Delta H_G = \ln |G|$) when the $|G|$ copies of $\bm{z}$-space
are disjoint in probability (i.e., $p_\theta^{\mathrm{AR}}$ spontaneously
breaks the symmetry), and zero when $p_\theta^{\mathrm{AR}}$ is already
$G$-invariant.  At $T_c$, where the Boltzmann distribution is exactly
$G$-invariant, the symmetrisation allows $p_\theta^{\mathrm{AR}}$ to
concentrate on one symmetry sector while the sum restores the full symmetry,
potentially yielding up to $\ln |G|$ bits of additional entropy.

\paragraph{Combining both approaches.}
The stochastic layers (Sec.~\ref{sec:stochastic-nf}) and the symmetry
surjection are complementary: the former injects entropy through noise, the
latter through symmetry averaging.  They can be composed: use a
$G$-symmetrised stochastic normalizing flow, gaining entropy from both
sources while maintaining a rigorous bound.

%======================================================================
\section{Comparison of approaches}
\label{sec:comparison}
%======================================================================

\begin{table}[h]
\centering
\caption{Comparison of token-based variational ans\"atze.}
\label{tab:comparison}
\begin{tabular}{lccc}
\toprule
\textbf{Approach}
  & \textbf{Rigorous bound on $F$?}
  & \textbf{Tractable $\ln q(\bm{\sigma})$?}
  & \textbf{Expressiveness} \\
\midrule
Raw-spin AR \cite{wu2019}
  & Yes & Exact & Limited by ordering \\
Block-spin AR (lossless)
  & Yes & Exact & Limited by block partition \\
VQ-VAE + AR prior
  & Only via loose joint bound & No & High \\
ELBO + importance sampling
  & Asymptotically & Estimated & High \\
\textbf{Discrete NF + AR base}
  & \textbf{Yes} & \textbf{Exact} & \textbf{High (learned bijection)} \\
\textbf{AR + $\beta$-annealing (AIS)}
  & \textbf{Yes} & \textbf{Via AIS weight} & \textbf{High (AR + MCMC)} \\
\textbf{NF + $\beta$-annealing}
  & \textbf{Yes} & \textbf{Via AIS weight} & \textbf{Higher (bijection + MCMC)} \\
\textbf{$G$-symmetrised NF}
  & \textbf{Yes} & \textbf{Exact (logsumexp)} & \textbf{High + symmetry} \\
\bottomrule
\end{tabular}
\end{table}

%======================================================================
\section{Haar wavelet basis for the 2D Ising model}
\label{sec:haar}
%======================================================================

The autoregressive models of the preceding sections factorise the
distribution over a 1D ordering of the lattice sites.  Any such ordering
maps the 2D lattice to a 1D chain, introducing long-range dependencies:
nearest neighbours on the lattice can be $O(L)$ apart in the 1D sequence.
A more natural approach is to work in a \emph{multiscale} basis that
respects the 2D structure.  The Haar wavelet provides a concrete, lossless
change of variables from spins to hierarchical coarse/detail coefficients,
with clean physical properties.

\subsection{Binary Haar wavelet transform}
\label{sec:haar-transform}

The standard Haar wavelet acts on real-valued signals.  For binary spins
$\sigma \in \{+1, -1\}$, the natural discrete analogue replaces
addition/subtraction with multiplication.

\paragraph{One level, 2$\times$2 block.}
Consider a $2 \times 2$ block of spins:
\begin{equation}
  \begin{pmatrix} \sigma_{11} & \sigma_{12} \\
                   \sigma_{21} & \sigma_{22} \end{pmatrix}
  \;\longleftrightarrow\;
  \begin{cases}
    c   = \sigma_{11}
      & \text{(coarse: anchor spin)}, \\
    d_1 = \sigma_{11}\,\sigma_{12}
      & \text{(horizontal detail)}, \\
    d_2 = \sigma_{11}\,\sigma_{21}
      & \text{(vertical detail)}, \\
    d_3 = \sigma_{11}\,\sigma_{22}
      & \text{(diagonal detail)}.
  \end{cases}
  \label{eq:haar-block}
\end{equation}
All four variables are binary $\{+1,-1\}$, and the map is lossless:
$\sigma_{12} = c\,d_1$, $\sigma_{21} = c\,d_2$,
$\sigma_{22} = c\,d_3$.  The detail variables $d_1, d_2, d_3$ are
\emph{bond variables}: $d_i = +1$ when the two spins are aligned,
$d_i = -1$ when anti-aligned.

\paragraph{Recursive application on a $2^n \times 2^n$ lattice.}
Apply the $2 \times 2$ transform to the full lattice, grouping spins into
non-overlapping blocks.  The coarse variables $c$ form a
$2^{n-1} \times 2^{n-1}$ lattice; the detail variables
$(d_1, d_2, d_3)$ form a $3 \times 2^{n-1} \times 2^{n-1}$ array.
Apply the same transform recursively to the coarse lattice.  After $n$
levels:
\begin{itemize}
  \item Level $n$ (coarsest): 1 coarse variable $c^{(n)}$.
  \item Level $k$ ($0 \leq k \leq n-1$): $3 \times 4^{n-1-k}$ detail
    variables $\bm{d}^{(k)}$, capturing the internal structure of each
    $2 \times 2$ block at scale~$k$.
\end{itemize}
The total variable count is $1 + 3 \sum_{j=0}^{n-1} 4^j = 4^n = N$.
The transform is a bijection $\{+1,-1\}^N \to \{+1,-1\}^N$.

\paragraph{Forward transform (spins $\to$ wavelet coefficients).}
\begin{enumerate}
  \item Set $\bm{s}^{(0)} \leftarrow \bm{\sigma}$ (the $2^n \times 2^n$
    spin array).
  \item For $k = 0, 1, \ldots, n-1$:
    \begin{enumerate}
      \item Partition $\bm{s}^{(k)}$ into non-overlapping $2 \times 2$
        blocks.
      \item For each block, compute $(c, d_1, d_2, d_3)$
        via~\eqref{eq:haar-block}.
      \item Store $\bm{d}^{(k)}$ (the detail array at level~$k$).
      \item Set $\bm{s}^{(k+1)} \leftarrow$ the array of coarse
        variables~$c$.
    \end{enumerate}
  \item Output $c^{(n)} = \bm{s}^{(n)}$ and
    $\{\bm{d}^{(0)}, \ldots, \bm{d}^{(n-1)}\}$.
\end{enumerate}

\paragraph{Inverse transform (wavelet coefficients $\to$ spins).}
\begin{enumerate}
  \item Set $\bm{s}^{(n)} \leftarrow c^{(n)}$.
  \item For $k = n-1, n-2, \ldots, 0$:
    \begin{enumerate}
      \item For each block at level~$k$, recover the four spins from the
        coarse variable $c$ (from $\bm{s}^{(k+1)}$) and the detail
        variables $(d_1, d_2, d_3)$ (from $\bm{d}^{(k)}$):
        $\sigma_{\mathrm{TL}} = c$,\;
        $\sigma_{\mathrm{TR}} = c\,d_1$,\;
        $\sigma_{\mathrm{BL}} = c\,d_2$,\;
        $\sigma_{\mathrm{BR}} = c\,d_3$.
      \item Assemble into $\bm{s}^{(k)}$.
    \end{enumerate}
  \item Output $\bm{\sigma} = \bm{s}^{(0)}$.
\end{enumerate}

\subsection{Energy decomposition in the Haar basis}
\label{sec:haar-energy}

The Ising energy $E = -J \sum_{\langle ij \rangle} \sigma_i \sigma_j$
decomposes into contributions from each level of the hierarchy.

\paragraph{Intra-block energy.}
For a $2 \times 2$ block at level~$k$, the four nearest-neighbour bonds
\emph{within} the block are
\begin{align}
  \sigma_{11}\sigma_{12} &= d_1, &
  \sigma_{11}\sigma_{21} &= d_2, \nonumber\\
  \sigma_{12}\sigma_{22} &= d_1 d_3, &
  \sigma_{21}\sigma_{22} &= d_2 d_3.
\end{align}
The intra-block energy is
\begin{equation}
  E_{\mathrm{intra}} = -J\big(d_1 + d_2 + d_1 d_3 + d_2 d_3\big)
  = -J(d_1 + d_2)(1 + d_3).
  \label{eq:haar-intra}
\end{equation}
The coarse variable $c$ \emph{does not appear}: the intra-block energy
depends only on the detail variables.  This is a consequence of the
$\mathbb{Z}_2$ symmetry---the energy is invariant under global spin flip,
and $c$ encodes the absolute orientation that the energy cannot see.

\paragraph{Inter-block energy.}
Consider two horizontally adjacent blocks $A$ and $B$ at level~$k$, with
coarse variables $c^A$ and $c^B$ and detail variables
$(d_1^A, d_2^A, d_3^A)$ and $(d_1^B, d_2^B, d_3^B)$.  The two
inter-block bonds (connecting the right column of $A$ to the left column
of $B$) are
\begin{align}
  \sigma_{12}^A \sigma_{11}^B &= c^A d_1^A \cdot c^B
    = (c^A c^B)\, d_1^A, \nonumber\\
  \sigma_{22}^A \sigma_{21}^B &= c^A d_3^A \cdot c^B d_2^B
    = (c^A c^B)\, d_3^A d_2^B.
  \label{eq:haar-inter}
\end{align}
The product $c^A c^B$ is a \emph{coarse-level bond variable}: at the next
level of the hierarchy, it equals the detail variable connecting the two
parent blocks.  The inter-block energy thus couples detail variables at
level~$k$ to coarse variables at level~$k+1$, but the coupling is
\emph{local} (involves only adjacent blocks) and \emph{known analytically}.

\paragraph{Total energy.}
The full Ising energy decomposes as
\begin{equation}
  E(\bm{\sigma}) = \sum_{k=0}^{n-1}
    \Big[\sum_{\text{blocks at level } k}
      E_{\mathrm{intra}}^{(k)}(\bm{d}^{(k)})
    + \sum_{\text{adjacent pairs}}
      E_{\mathrm{inter}}^{(k)}(\bm{d}^{(k)},\, \bm{c}^{(k+1)})
    \Big],
  \label{eq:haar-energy-total}
\end{equation}
where each term is local and analytically known.  The coarsest variable
$c^{(n)}$ does not appear in the energy at all.

\subsection{Autoregressive factorisation in the Haar basis}
\label{sec:haar-ar}

The Haar wavelet defines a natural coarse-to-fine autoregressive
factorisation:
\begin{equation}
  p(\bm{\sigma})
  = p(c^{(n)})
  \times \prod_{k=n-1}^{0}
    p\!\big(\bm{d}^{(k)} \,\big|\, c^{(n)},\,
      \bm{d}^{(n-1)}, \ldots, \bm{d}^{(k+1)}\big).
  \label{eq:haar-ar}
\end{equation}
Each factor conditions on all coarser levels.  The log-probability is
\begin{equation}
  \ln p(\bm{\sigma})
  = \ln p(c^{(n)})
  + \sum_{k=n-1}^{0}
    \ln p\!\big(\bm{d}^{(k)} \,\big|\, \text{coarser levels}\big),
  \label{eq:haar-logprob}
\end{equation}
which is exact and tractable (a sum of $n$ terms).

\paragraph{Structure of the conditionals.}
At level~$k$, the conditional $p(\bm{d}^{(k)} \mid \text{coarser levels})$
is a distribution over $3 \times 2^{n-k-1} \times 2^{n-k-1}$ binary
variables.  The energy decomposition~\eqref{eq:haar-energy-total} shows
that this conditional depends on:
\begin{enumerate}
  \item The \emph{intra-block} energy at level~$k$: depends only on
    $\bm{d}^{(k)}$, local within each block.
  \item The \emph{inter-block} energy at level~$k$: couples detail
    variables in adjacent blocks, with coefficients determined by the
    coarse variables $\bm{c}^{(k+1)}$ (which are known from the coarser
    levels).
\end{enumerate}
Crucially, the inter-block coupling is \emph{short-range}: it connects
only nearest-neighbour blocks.  Conditioned on the coarse variables, the
detail variables at level~$k$ form a 2D system with local interactions---
a much simpler object than the original $N$-spin Ising model.

\paragraph{The coarsest level.}
For a $\mathbb{Z}_2$-symmetric system, $p(c^{(n)}) = \tfrac{1}{2}$
(uniform over $\{+1, -1\}$), contributing exactly $\ln 2$ to the entropy.
This single bit of entropy is \emph{structurally guaranteed}---it cannot
be lost to mode collapse.

\paragraph{Within-block factorisation.}
The three detail variables $(d_1, d_2, d_3)$ within each block are
correlated (through the intra-block energy~\eqref{eq:haar-intra}).  They
can be factorised autoregressively:
\begin{equation}
  p(d_1, d_2, d_3 \mid \text{context})
  = p(d_1 \mid \text{ctx})\;
    p(d_2 \mid d_1, \text{ctx})\;
    p(d_3 \mid d_1, d_2, \text{ctx}),
  \label{eq:haar-within-block}
\end{equation}
where ``context'' denotes the coarse variables and detail variables from
adjacent blocks.  Each factor is a single binary conditional, parameterised
by a neural network.

\subsection{Implementation plan}
\label{sec:haar-implementation}

We describe a concrete implementation for the $2^n \times 2^n$ Ising model.

\paragraph{Architecture: scale-equivariant ConvNet.}
At each level~$k$, the conditional
$p(\bm{d}^{(k)} \mid \text{coarser levels})$ is parameterised by a
convolutional neural network $f_\theta^{(k)}$ that takes as input the
coarse-level information and outputs the conditional probabilities for the
detail variables.

At the critical point, the RG fixed-point structure implies that the
conditional has the \emph{same functional form} at every level (up to
rescaling).  This motivates \emph{weight sharing}: a single ConvNet
$f_\theta$ is used at all levels, with the scale index $k$ provided as an
additional input (e.g., via a scale embedding added to the feature maps).
This reduces the parameter count from $O(n)$ networks to $O(1)$.

The ConvNet operates on the $2^{n-k-1} \times 2^{n-k-1}$ grid of blocks
at level~$k$.  Its input channels are:
\begin{itemize}
  \item The coarse variables $\bm{c}^{(k+1)}$ (1 channel,
    $2^{n-k-1} \times 2^{n-k-1}$).
  \item Upsampled coarser-level information (optional, for capturing
    longer-range context).
\end{itemize}
Its output channels are the parameters of the within-block autoregressive
conditionals~\eqref{eq:haar-within-block}: 3 logits per block (one for
each of $d_1$, $d_2$, $d_3$, with $d_2$ and $d_3$ conditioned on
earlier detail variables via masking).

\paragraph{Sampling algorithm.}
\begin{enumerate}
  \item Sample $c^{(n)} \sim \mathrm{Uniform}\{+1, -1\}$.
  \item For $k = n-1, n-2, \ldots, 0$:
    \begin{enumerate}
      \item Compute the coarse array $\bm{c}^{(k+1)}$ from the already-
        generated coarser levels (via the inverse transform applied down
        to level $k+1$).
      \item Run the ConvNet $f_\theta$ on $\bm{c}^{(k+1)}$ (with scale
        embedding $k$) to obtain logits for $\bm{d}^{(k)}$.
      \item Sample $\bm{d}^{(k)}$ autoregressively within each block:
        $d_1 \to d_2 \to d_3$.
    \end{enumerate}
  \item Apply the full inverse Haar transform to recover
    $\bm{\sigma} \in \{+1,-1\}^N$.
\end{enumerate}
The total cost is $n$ ConvNet evaluations (one per level), each on a grid
of decreasing size.  The dominant cost is level~$0$ (finest), which
operates on a $2^{n-1} \times 2^{n-1}$ grid.

\paragraph{Log-probability computation.}
Given a spin configuration $\bm{\sigma}$:
\begin{enumerate}
  \item Apply the forward Haar transform to obtain
    $\{c^{(n)}, \bm{d}^{(n-1)}, \ldots, \bm{d}^{(0)}\}$.
  \item Compute $\ln p(c^{(n)}) = -\ln 2$.
  \item For $k = n-1, \ldots, 0$: run the ConvNet on $\bm{c}^{(k+1)}$
    to obtain logits, then evaluate
    $\ln p(\bm{d}^{(k)} \mid \text{coarser levels})$ as a sum of
    per-block, per-variable log-probabilities.
  \item Sum: $\ln p(\bm{\sigma}) = -\ln 2 + \sum_k \ln p(\bm{d}^{(k)}
    \mid \text{coarser levels})$.
\end{enumerate}
This gives an \emph{exact} log-probability, suitable for the variational
free energy bound or the AIS weight.

\paragraph{Training.}
The model can be trained by any of the methods discussed in this paper:
\begin{itemize}
  \item \textbf{Reverse KL (REINFORCE):} minimise
    $\Fvar[p_\theta] = \E[E(\bm{\sigma}) + T \ln p_\theta(\bm{\sigma})]$.
    The hierarchical structure mitigates mode collapse: the coarsest level
    has fixed entropy $\ln 2$, and the shared ConvNet prevents any single
    level from collapsing independently.
  \item \textbf{Forward KL (MLE on MCMC samples):} minimise
    $-\E_{p_{\mathrm{Boltz}}}[\ln p_\theta(\bm{\sigma})]$.  The
    coarse-to-fine structure means the network at each level sees
    \emph{local} detail variables conditioned on coarser context---a
    simpler learning problem than the flat AR model.
  \item \textbf{$\beta$-annealing (AIS):} use the Haar-basis AR model as
    the base $p_\theta$ in the AIS framework of
    Sec.~\ref{sec:markov-corrected}.  The hierarchical base captures
    short-range correlations; the AIS chain (with Wolff updates) builds
    the long-range correlations.
\end{itemize}

\paragraph{Complexity comparison.}
\begin{center}
\begin{tabular}{lcc}
\toprule
& \textbf{Snake-order AR} & \textbf{Haar-basis AR} \\
\midrule
Autoregressive depth & $N = L^2$ & $n = \log_2 L$ levels \\
Conditional range & $O(L)$ & $O(1)$ (local) \\
$\mathbb{Z}_2$ symmetry & Must be learned & Exact ($c^{(n)}$ decouples) \\
At $T_c$ & Global dependence & Self-similar across levels \\
Network evaluations & $N$ (or 1 masked) & $n$ ConvNets \\
Weight sharing & None & Across all $n$ levels \\
\bottomrule
\end{tabular}
\end{center}

\paragraph{Relation to the coupling layers.}
The checkerboard partition used in the discrete coupling layers
(Sec.~\ref{sec:coupling}) is precisely one level of the Haar
decomposition: the $A$ sublattice plays the role of the coarse variables,
and the $B$ sublattice plays the role of the detail variables.  Stacking
$n$ coupling layers with progressively coarser checkerboard partitions
recovers the full Haar hierarchy.  The Haar-basis AR model makes this
multiscale structure explicit and allows weight sharing across levels,
which the flat coupling-layer stack does not.

\paragraph{Practical assessment: concentration at the finest level.}
The token count at each level is $1 + 3 + 12 + \cdots + 3(L/2)^2 = L^2$.
The finest level alone contributes $3(L/2)^2 = 3L^2/4$, i.e.\ \emph{75\%
of all degrees of freedom}, independent of~$L$.  The coarser levels
($s_0, d_1, \ldots, d_{n-2}$) together account for only $L^2/4$ variables.
Consequently, the multi-scale decomposition does not reduce the
dimensionality of the hard modelling problem---it reorganises it.

The value of the Haar basis is therefore not in simplifying the finest
level, but in providing \emph{structured conditioning context}: when
modelling $p(\bm{d}^{(0)} \mid \text{coarser levels})$, the coarse
variables supply a compressed summary of the global domain structure.
For small lattices ($L \leq 32$), a flat MADE with sufficient hidden
units can capture the same correlations; the Haar basis becomes
advantageous at large~$L$ where the correlation length at~$T_c$ exceeds
the effective receptive field of a flat autoregressive model.

The multi-scale decomposition is also valuable as a \emph{diagnostic}:
the scale-by-scale conditional entropy
$H(\bm{d}^{(k)} \mid \text{coarser levels})$ reveals which length
scales carry the most uncertainty, providing a direct probe of the
RG structure that flat models do not offer.

A reference implementation of the bijective Haar transform for
$\{+1,-1\}$ spins is provided in \texttt{dsflow\_ising/multiscale.py}.

\paragraph{Connection to multi-scale autoregressive models in vision.}
The coarse-to-fine autoregressive factorisation~\eqref{eq:haar-ar} is
closely related to the Visual Autoregressive Modeling (VAR)
framework~\cite{tian2024var}, which replaces next-token prediction with
next-scale prediction for image generation.  VAR uses a multi-scale
VQVAE tokeniser with additive residuals in continuous feature space;
the Haar basis provides the discrete analogue, with multiplicative
residuals that keep all variables in $\{+1,-1\}$.  The Variational
Lossy Autoencoder~\cite{chen2017vlae} provides a complementary
perspective: by limiting the autoregressive decoder's receptive field,
it forces global information into the latent code---analogous to how
the Haar hierarchy forces long-range structure into the coarse levels.

%======================================================================
\section{Suggested numerical experiments}
\label{sec:experiments}
%======================================================================

As a concrete starting point, we propose the following experiments on the
\textbf{2D Ising model} on an $L \times L$ square lattice with periodic
boundary conditions:
\begin{equation}
  E(\bm{\sigma}) = -J \sum_{\langle ij \rangle} \sigma_i \sigma_j.
\end{equation}

\paragraph{Architecture.}
\begin{itemize}
  \item \textbf{Base model:} MADE network over $\bm{z} \in \{\pm 1\}^N$
    with raster-scan ordering, hidden layers of width $4N$.
  \item \textbf{Flow:} $L_{\mathrm{flow}} = 4$--$8$ coupling layers with
    alternating checkerboard partitions; each mask network is a small
    ConvNet (2--3 layers, $3\times 3$ kernels).
  \item \textbf{System sizes:} $8 \times 8$, $16 \times 16$,
    $32 \times 32$.
\end{itemize}

\paragraph{Training.}
STE for flow gradients, REINFORCE with learned baseline for base-model
gradients.  Sweep temperatures across $T_c \approx 2.269\, J/k_B$.

\paragraph{Diagnostics.}
\begin{enumerate}
  \item Compare $\Fvar[q]$ against exact results (transfer matrix for
    small $L$, Monte Carlo for larger $L$).
  \item Plot base-distribution entropy $H[p_{\theta}]$ and
    layer-by-layer free-energy reduction $\Delta F_l$ as functions of
    $T$.
  \item Feed configurations with known defect content (single domain
    wall, vortex pair) through $f_\phi^{-1}$; visualize the latent
    representation.
  \item Measure the minimum flow depth $L^\ast$ needed for a target
    accuracy $|\Fvar - \Ftrue| / N < \epsilon$ and plot $L^\ast(T)$.
  \item At $T_c$, compare the learned flow with the Kramers--Wannier
    transformation on test configurations.
\end{enumerate}

%======================================================================
\section{Related work}
\label{sec:related}
%======================================================================

The present proposal sits at the intersection of three lines of work:
autoregressive variational methods for statistical mechanics, normalizing
flows for lattice field theory, and discrete generative models in machine
learning.  We survey each in turn and identify the gap that the discrete
flow framework fills.

\subsection{Autoregressive variational ans\"atze for spin systems}

Wu, Wang, and Zhang~\cite{wu2019} introduced the use of autoregressive
neural networks (PixelCNN, MADE) as variational ans\"atze for classical
statistical mechanics, with exact log-probabilities enabling a rigorous
variational free-energy bound.  Subsequent work has refined the
autoregressive architecture: Biazzo, Wu, and
Carleo~\cite{biazzo2024sparse} proposed the TwoBo architecture,
incorporating knowledge of the two-body interaction structure into sparse
autoregressive networks for frustrated systems with $>$1000 spins.
Bialas, Korcyl, and Stebel~\cite{bialas2022hierarchical} introduced
hierarchical associations between spins and neurons, achieving scaling
with the linear extent~$L$ rather than the total number of spins.
Pan and Zhang~\cite{pan2024messagepassingvan} augmented variational
autoregressive networks with message passing to better capture spin-spin
interactions.  All of these works use \emph{purely autoregressive}
distributions without flow layers; the expressiveness is limited by the
autoregressive factorization itself.

\subsection{Normalizing flows for lattice field theory}

Albergo, Kanwar, and Shanahan~\cite{albergo2019} pioneered the use of
normalizing flows (specifically RealNVP affine coupling layers with
checkerboard masking) for lattice field theory, demonstrating the method
on $\phi^4$ scalar theory in 2D with \emph{continuous} field variables.
Kanwar et al.~\cite{kanwar2020} extended this to gauge-equivariant flows
for U(1) lattice gauge theory.  Nicoli et
al.~\cite{nicoli2020,nicoli2021} showed that normalizing-flow samplers
can estimate absolute free energies (not just differences) for
continuous-field theories.  Comprehensive reviews of this programme are
given in Refs.~\cite{albergo2021tutorial,cranmer2023review}.  A key
limitation of this entire line of work is its restriction to
\emph{continuous} degrees of freedom; discrete spin models are not
directly addressed.

\subsection{Neural network renormalization group}

Li and Wang~\cite{li2018neuralrg} proposed the Neural Network
Renormalization Group (NeuralRG), which uses normalizing flows in a
hierarchical, multiscale architecture motivated by the RG.  The
variational free energy provides a rigorous upper bound.  Applied to the
2D Ising model, the method uses RealNVP coupling layers and therefore
operates with a \emph{continuous relaxation} of the binary spin
variables, rather than natively discrete transformations.

\subsection{Discrete normalizing flows in machine learning}

Tran et al.~\cite{tran2019} introduced discrete normalizing flows with
two architectures: discrete autoregressive flows and discrete bipartite
flows.  The bipartite flow uses a modular location-scale transform
$y_d = (\mu_d + \sigma_d x_d) \bmod K$ that reduces to XOR for binary
variables---precisely the coupling layer~\eqref{eq:coupling-layer} used
here.  They tested discrete autoregressive flows on the 2D $q$-state
Potts model ($q = 3, 4, 5$; lattices of $3 \times 3$ and $4 \times 4$
spins), training by maximum likelihood on Metropolis--Hastings samples.
The flow improved NLL over the autoregressive baseline in most settings,
with the largest gains at weak coupling ($J = 0.1$) and larger systems,
consistent with the entropy-preservation analysis of
Sec.~\ref{sec:forward-kl}: forward KL training maintains high entropy in
the base, creating the conditions under which the flow can rearrange
probability mass to lower energy.  However, their work frames the
problem as \emph{density estimation}, not variational inference: there is
no free-energy objective, no rigorous bound on $F$, no comparison to
exact partition functions, and no physical analysis of the learned flow.
The systems studied are also very small (9--16 spins), and the flow
network is a lookup table that cannot scale to larger lattices.
Hoogeboom et al.~\cite{hoogeboom2019} proposed Integer
Discrete Flows for lossless compression, using additive coupling with
rounding for ordinal data, but did not target spin systems.

\subsection{Boltzmann generators}

No\'e et al.~\cite{noe2019} introduced Boltzmann generators---normalizing
flows trained on the energy function to sample Boltzmann distributions
for molecular systems.  This work operates entirely in continuous
configuration space (molecular coordinates) and does not address discrete
degrees of freedom.

\subsection{Discrete flow matching and diffusion for spin systems}

More recently, Tuo et al.~\cite{tuo2025} applied discrete flow matching
to the Ising model, learning continuous-time transport maps from noisy
distributions to the Boltzmann distribution.  This is fundamentally
different from the bijective coupling-layer approach: flow matching does
not use invertible transformations and does not provide the same type of
exact variational bounds.  Ghio et al.~\cite{ghio2024} provided a
theoretical analysis of flows, diffusion, and autoregressive methods for
spin glasses, showing that these methods can encounter first-order phase
transitions along the generative path that impede sampling---an important
caveat for any flow-based approach in the glassy regime.

\subsection{Positioning of the present proposal}

Table~\ref{tab:literature} summarises the landscape.  Each existing
approach addresses a subset of the desiderata; the present proposal
combines discrete bijectivity (from~\cite{tran2019}), the variational
free-energy framework (from~\cite{wu2019}), and the physical
interpretability of normalizing flows (from~\cite{li2018neuralrg,
albergo2019}).  What is new is the synthesis: discrete coupling layers
operating \emph{natively} on binary spins, composed with an
autoregressive base, trained via the variational free energy with
rigorous bounds, and analysed for physical content (flow depth scaling,
duality, defect encoding).

\begin{table}[h]
\centering
\caption{Positioning of the discrete NF + AR base proposal relative to
existing work.}
\label{tab:literature}
\begin{tabular}{lcccc}
\toprule
& \textbf{Discrete spins} & \textbf{Bijective flow}
& \textbf{Rigorous $F$ bound} & \textbf{Learned transform} \\
\midrule
Wu et al.~\cite{wu2019} & \checkmark & --- & \checkmark & --- \\
Albergo et al.~\cite{albergo2019} & --- & \checkmark & \checkmark & \checkmark \\
Li \& Wang~\cite{li2018neuralrg} & relaxed & \checkmark & \checkmark & \checkmark \\
Tran et al.~\cite{tran2019} & \checkmark & \checkmark & --- & \checkmark \\
Tuo et al.~\cite{tuo2025} & \checkmark & --- & --- & \checkmark \\
\textbf{This proposal} & \checkmark & \checkmark & \checkmark & \checkmark \\
\bottomrule
\end{tabular}
\end{table}

%======================================================================
\section{Outlook}
\label{sec:outlook}
%======================================================================

Several extensions are immediate:
\begin{itemize}
  \item \textbf{$q$-state Potts model.} The coupling layers generalize
    to conditional permutations of $\{1, \ldots, q\}$, parameterized by
    a network that outputs a permutation matrix (or its Gumbel--Sinkhorn
    relaxation).
  \item \textbf{Continuous spins (XY, Heisenberg).} Standard continuous
    normalizing flows apply directly, with the full machinery of
    coupling layers and affine transformations.
  \item \textbf{Lattice gauge theories.} The coupling layers can be
    adapted to respect gauge symmetry by acting on gauge-invariant
    combinations (plaquettes, Wilson loops), following the approach of
    Albergo et al.
  \item \textbf{Quantum systems.} The variational free energy can be
    replaced by the variational energy of a quantum Hamiltonian, with the
    autoregressive model representing an amplitude (autoregressive neural
    quantum state).
\end{itemize}

The discrete normalizing flow framework provides a principled,
rigorous, and physically interpretable generalization of autoregressive
variational methods.  The learned bijection is not merely a
computational device---it is a window into the structure of the
Boltzmann distribution, encoding renormalization, duality, and the
organization of topological defects.

\begin{thebibliography}{99}

\bibitem{wu2019}
D.~Wu, L.~Wang, and P.~Zhang,
``Solving statistical mechanics using variational autoregressive
networks,''
\textit{Phys.\ Rev.\ Lett.}\ \textbf{122}, 080602 (2019).
[arXiv:1809.10606]

\bibitem{biazzo2024sparse}
I.~Biazzo, D.~Wu, and G.~Carleo,
``Sparse autoregressive neural networks for classical spin systems,''
\textit{Mach.\ Learn.: Sci.\ Technol.}\ \textbf{5}, 045001 (2024).
[arXiv:2402.16579]

\bibitem{bialas2022hierarchical}
P.~Bialas, P.~Korcyl, and T.~Stebel,
``Hierarchical autoregressive neural networks for statistical systems,''
\textit{Comput.\ Phys.\ Commun.}\ \textbf{281}, 108502 (2022).

\bibitem{pan2024messagepassingvan}
F.~Pan and P.~Zhang,
``Message passing variational autoregressive network for solving
intractable Ising models,''
\textit{Commun.\ Phys.}\ \textbf{7}, 205 (2024).

\bibitem{albergo2019}
M.~S.~Albergo, G.~Kanwar, and P.~E.~Shanahan,
``Flow-based generative models for Markov chain Monte Carlo in lattice
field theory,''
\textit{Phys.\ Rev.\ D}\ \textbf{100}, 034515 (2019).
[arXiv:1904.12072]

\bibitem{kanwar2020}
G.~Kanwar, M.~S.~Albergo, D.~Boyda, K.~Cranmer, D.~C.~Hackett,
S.~Racani\`ere, D.~J.~Rezende, and P.~E.~Shanahan,
``Equivariant flow-based sampling for lattice gauge theory,''
\textit{Phys.\ Rev.\ Lett.}\ \textbf{125}, 121601 (2020).
[arXiv:2003.06413]

\bibitem{nicoli2020}
K.~A.~Nicoli, S.~Nakajima, N.~Strodthoff, W.~Samek, K.-R.~M\"uller,
and P.~Kessel,
``Asymptotically unbiased estimation of physical observables with neural
samplers,''
\textit{Phys.\ Rev.\ E}\ \textbf{101}, 023304 (2020).
[arXiv:1910.13496]

\bibitem{nicoli2021}
K.~A.~Nicoli, C.~J.~Anders, L.~Funcke, T.~Hartung, K.~Jansen,
S.~Kessel, S.~Nakajima, and P.~Stornati,
``Estimation of thermodynamic observables in lattice field theories with
deep generative models,''
\textit{Phys.\ Rev.\ Lett.}\ \textbf{126}, 032001 (2021).
[arXiv:2007.07115]

\bibitem{albergo2021tutorial}
M.~S.~Albergo, D.~Boyda, D.~C.~Hackett, G.~Kanwar, K.~Cranmer,
S.~Racani\`ere, D.~J.~Rezende, and P.~E.~Shanahan,
``Introduction to normalizing flows for lattice field theory,''
arXiv:2101.08176 (2021).

\bibitem{cranmer2023review}
K.~Cranmer, G.~Kanwar, S.~Racani\`ere, D.~J.~Rezende, and
P.~E.~Shanahan,
``Advances in machine-learning-based sampling motivated by lattice
quantum chromodynamics,''
\textit{Nat.\ Rev.\ Phys.}\ \textbf{5}, 526--535 (2023).

\bibitem{li2018neuralrg}
S.-H.~Li and L.~Wang,
``Neural network renormalization group,''
\textit{Phys.\ Rev.\ Lett.}\ \textbf{121}, 260601 (2018).
[arXiv:1802.02840]

\bibitem{tran2019}
D.~Tran, K.~Vafa, K.~K.~Agrawal, L.~Dinh, and B.~Poole,
``Discrete flows: Invertible generative models of discrete data,''
in \textit{Advances in Neural Information Processing Systems~32}
(NeurIPS 2019).
[arXiv:1905.10347]

\bibitem{hoogeboom2019}
E.~Hoogeboom, J.~W.~T.~Peters, R.~van~den~Berg, and M.~Welling,
``Integer discrete flows and lossless compression,''
in \textit{Advances in Neural Information Processing Systems~32}
(NeurIPS 2019).
[arXiv:1905.07376]

\bibitem{noe2019}
F.~No\'e, S.~Olsson, J.~K\"ohler, and H.~Wu,
``Boltzmann generators: Sampling equilibrium states of many-body systems
with deep learning,''
\textit{Science}\ \textbf{365}, eaaw1147 (2019).
[arXiv:1812.01729]

\bibitem{tuo2025}
H.~Tuo, H.~Zeng, Y.~Chen, and L.~Cheng,
``Scalable multitemperature free energy sampling of classical Ising spin
states,''
arXiv:2503.08063 (2025).

\bibitem{ghio2024}
D.~Ghio, Y.~M.~Dandi, F.~Krzakala, and L.~Zdeborov\'a,
``Sampling with flows, diffusion, and autoregressive neural networks:
A spin-glass perspective,''
\textit{Proc.\ Natl.\ Acad.\ Sci.\ USA}\ \textbf{121}, e2311810121
(2024).
[arXiv:2308.14085]

\bibitem{neal2001ais}
R.~M.~Neal,
``Annealed importance sampling,''
\textit{Stat.\ Comput.}\ \textbf{11}, 125--139 (2001).
[arXiv:physics/9803008]

\bibitem{wu2020snf}
H.~Wu, J.~K\"ohler, and F.~No\'e,
``Stochastic normalizing flows,''
in \textit{Advances in Neural Information Processing Systems~33}
(NeurIPS 2020), pp.~5933--5944.
[arXiv:2002.06707]

\bibitem{jarzynski1997}
C.~Jarzynski,
``Nonequilibrium equality for free energy differences,''
\textit{Phys.\ Rev.\ Lett.}\ \textbf{78}, 2690--2693 (1997).
[arXiv:cond-mat/9610209]

\bibitem{tian2024var}
K.~Tian, Y.~Jiang, Z.~Yuan, B.~Peng, and L.~Wang,
``Visual autoregressive modeling: Scalable image generation via
next-scale prediction,''
in \textit{Advances in Neural Information Processing Systems~37}
(NeurIPS 2024).
[arXiv:2404.02905]

\bibitem{chen2017vlae}
X.~Chen, D.~P.~Kingma, T.~Salimans, Y.~Duan, P.~Dhariwal,
J.~Schulman, I.~Sutskever, and P.~Abbeel,
``Variational lossy autoencoder,''
in \textit{International Conference on Learning Representations}
(ICLR 2017).
[arXiv:1611.02731]

\end{thebibliography}

\end{document}
